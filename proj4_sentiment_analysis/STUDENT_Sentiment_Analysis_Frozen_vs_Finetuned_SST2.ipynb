{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4cb75c",
   "metadata": {},
   "source": [
    "\n",
    "# Sentiment Analysis: Frozen Encoder vs Full Fineâ€‘Tuning (SSTâ€‘2)\n",
    "\n",
    "**Objective:** Compare a **head only fine tuned encoder** to a **fully fineâ€‘tuned encoder**  for sentiment classification on **GLUE/SSTâ€‘2**.\n",
    "\n",
    "You'll:\n",
    "- Load the **SSTâ€‘2** dataset from ðŸ¤— `datasets`\n",
    "- Tokenize with a Hugging Face encoder (`distilbert-base-uncased` by default)\n",
    "- Train a **head only fine tuned encoder** (encoder frozen, only the classification head trained)\n",
    "- Train a **fully fineâ€‘tuned encoder** model (encoder + head fine tuned)\n",
    "- Evaluate both models and visualize the improvement\n",
    "\n",
    "> ðŸ’¡ *Why this design?*  \n",
    "> Testing a **frozen encoder** quantifies how much task signal a generalâ€‘purpose language model already encodes. **Full fineâ€‘tuning** measures the benefit of adapting the encoder to the task.\n",
    "\n",
    "<mark>**Note: Possible 25 point penalty here!**<br>\n",
    "Please remove all repetitive code (this especially applies to the TrainingArguments and Trainer), if you are typing code twice then refactor into a function.<br>\n",
    "Please do not use any code that you do not understand<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8604e9",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup\n",
    "\n",
    "Run the cell below to install dependencies (if needed) and set a reproducible environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc2f095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: evaluate in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (3.10.7)\n",
      "Requirement already satisfied: filelock in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: anyio in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: networkx in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.9.86)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./miniconda3/envs/torch_env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Torch: 2.5.1+cu121 | CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If you're running in a fresh environment, uncomment the installs.\n",
    "%pip install -U transformers datasets accelerate evaluate scikit-learn matplotlib\n",
    "\n",
    "import os, random, math, time, json\n",
    "import numpy as np\n",
    "\n",
    "# Set the environment variable to use the first GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0115e02",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration\n",
    "\n",
    "You can tweak model and training settings here. When short on compute, enable **SUBSET_FRACTION** to run a quick demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec64cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model_name\": \"distilbert-base-uncased\",\n",
      "  \"num_labels\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"per_device_train_batch_size\": 32,\n",
      "  \"per_device_eval_batch_size\": 64,\n",
      "  \"epochs_headonly_finetune\": 2,\n",
      "  \"epochs_full_finetune\": 2,\n",
      "  \"learning_rate_headonly_finetune\": 0.0005,\n",
      "  \"learning_rate_full_finetune\": 2e-05,\n",
      "  \"weight_decay\": 0.01,\n",
      "  \"warmup_ratio\": 0.06,\n",
      "  \"subset_fraction\": null,\n",
      "  \"output_dir\": \"checkpoints_sst2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"distilbert-base-uncased\",  # small, fast baseline\n",
    "    \"num_labels\": 2,\n",
    "    \"max_length\": 128,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"per_device_eval_batch_size\": 64,\n",
    "    \"epochs_headonly_finetune\": 2,      # linear-probe epochs\n",
    "    \"epochs_full_finetune\": 2,   # full finetune epochs\n",
    "    \"learning_rate_headonly_finetune\": 5e-4,     # a bit higher since only head trains\n",
    "    \"learning_rate_full_finetune\": 2e-5,  # standard for full finetune\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "    \"subset_fraction\": None,   # set to None for full dataset; e.g., 0.3 uses 30% for quicker runs\n",
    "    \"output_dir\": \"checkpoints_sst2\"\n",
    "}\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4e84d",
   "metadata": {},
   "source": [
    "\n",
    "## 2) (20 pts) Load the SSTâ€‘2 Dataset\n",
    "\n",
    "SSTâ€‘2 (from GLUE) is a **binary** sentiment task (positive/negative). It comes with 3 splits; train, validation and test.<br>\n",
    "<mark>Ignore the test set as it has hidden labels.  Generate a new test set from part of the train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be80540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c01a7396ae47f995e473844fd10764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df33c3fa41554e30aa6c7bb677ff3993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310af690ac25473cbb135970bc3e3db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1dd85f02f5445999f7711fd27a90d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52c3465d1634eacbb24594579fc4634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4184034725fb4eefb93697e8f968f5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd203d6fd4d54d3b9bd1d525a0f129fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 60614\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 6735\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 872\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "#your code here\n",
    "# The 'test' split has no labels, so weâ€™ll re-split the 'train' set.\n",
    "# We'll keep 90% for training and 10% for testing to simulate a labeled test set.\n",
    "raw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
    "\n",
    "# Keep the validation split as provided by GLUE (for dev evaluation)\n",
    "val_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "\n",
    "print(raw_datasets)\n",
    "print(val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70911bee",
   "metadata": {},
   "source": [
    "\n",
    "## 3) (10 points) Tokenization\n",
    "\n",
    "Use the DistilBERT tokenizer and truncate/pad to a max_length defined in CONFIG above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64e8219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cd3d12fb20413fbbb9b9443e5f5493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bfdde851ce4e76a733b7186ace930e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ec2d2757784345a70b3a7b3fb33c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4317e0173a074a0298b0cd3fb79ccb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddbbae3e8ca4606acb2a709b6b302dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60614 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a137972f865f4141a4949b4a1178624d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9a1d24fcd647ccb70c8acc6bf817ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer for the model specified in CONFIG\n",
    "# Tokenize all splits of the dataset using the tokenize_fn function\n",
    "# - Truncate sequences to max_length (128 tokens)\n",
    "# - Use DataCollatorWithPadding for dynamic padding during batching\n",
    "# Define label names and extract num_labels for the classification task\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], use_fast=True)\n",
    "\n",
    "#your code here\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize input text with truncation and padding handled later by DataCollator\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, max_length=CONFIG[\"max_length\"])\n",
    "\n",
    "# Apply tokenizer to all splits\n",
    "tokenized_datasets = {}\n",
    "for split_name, dataset in raw_datasets.items():\n",
    "    tokenized_datasets[split_name] = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets[\"train\"].set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_datasets[\"test\"].set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703f48c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) (10 pts) Metrics\n",
    "\n",
    "Track accuracy and F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb14df92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9882649c1316459c9726c8cbacbafb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cce53d12bc4db49b909647b2f25b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define accuracy and F1 metrics using the evaluate library, and create a compute_metrics function\n",
    "# that takes model predictions and labels (A tuple (logits, labels) from the Trainer evaluation loop), \n",
    "# computes predicted classes, and returns a dictionary\n",
    "# with with 'accuracy' and 'f1' scores for use in Hugging Face Trainer evaluation.\n",
    "\n",
    "# your code here\n",
    "# Load metrics from the evaluate library\n",
    "metric_accuracy = evaluate.load(\"accuracy\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy and F1 from model predictions.\n",
    "    eval_pred is a tuple (logits, labels) automatically provided by Trainer.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # Convert model logits to predicted class indices\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = metric_accuracy.compute(predictions=preds, references=labels)\n",
    "    f1 = metric_f1.compute(predictions=preds, references=labels)\n",
    "    \n",
    "    # Return a combined dictionary (Hugging Face Trainer expects this format)\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c4b9ad",
   "metadata": {},
   "source": [
    "## 5) (10 pts) Is accuracy a good metric for this task?  Why or why not?\n",
    "\n",
    "Back up your answer with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3558cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1: 33878 samples (55.89%)\n",
      "Label 0: 26736 samples (44.11%)\n",
      "\n",
      "Class balance ratio: 0.789\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from collections import Counter\n",
    "\n",
    "# Count how many positive (1) and negative (0) samples are in the training data\n",
    "label_counts = Counter(raw_datasets[\"train\"][\"label\"])\n",
    "total = sum(label_counts.values())\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} samples ({count/total:.2%})\")\n",
    "\n",
    "# Compute class balance ratio\n",
    "balance_ratio = min(label_counts.values()) / max(label_counts.values())\n",
    "print(f\"\\nClass balance ratio: {balance_ratio:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cf5e1",
   "metadata": {},
   "source": [
    "\n",
    "## 6) (20 pts) First Stage **fine tuned head only with Frozen Encoder**\n",
    "\n",
    "- Load `AutoModelForSequenceClassification` for CONFIG.num_labels labels.\n",
    "- **Freeze** all encoder layers so only the classification head trains.\n",
    "- Train for CONFIG.epochs_headonly_finetune and evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7f857d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model=None, encoder_requires_grad=False):\n",
    "    \"\"\"\n",
    "    Build or reuse a sequence classification model and (un)freeze its encoder.\n",
    "\n",
    "    Args:\n",
    "        model: An existing Hugging Face sequence classification model. If None,\n",
    "               a model is loaded from CONFIG[\"model_name\"] with `num_labels`.\n",
    "        encoder_requires_grad (bool): If False, freeze the encoder (linear probe).\n",
    "                                      If True, unfreeze the encoder (full finetune).\n",
    "\n",
    "    Returns:\n",
    "        The model with its encoder parameters' requires_grad set accordingly\n",
    "        (only applied when the backbone is DistilBERT and accessible via\n",
    "        `model.distilbert`).\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes a DistilBERT-based classifier where the encoder\n",
    "          module is exposed as `model.distilbert`.\n",
    "        - If the provided model does not have a `distilbert` attribute, no\n",
    "          parameters are modified.\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    if model is None:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CONFIG[\"model_name\"], num_labels=CONFIG[\"num_labels\"]\n",
    "        )\n",
    "\n",
    "    if hasattr(model, \"distilbert\"):\n",
    "        for param in model.distilbert.parameters():\n",
    "            param.requires_grad = encoder_requires_grad\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,} | Frozen parameters: {frozen_params:,}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab16c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher_bonillajulien_22/miniconda3/envs/torch_env/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb4c956af6348aa8a4ff9844bbecd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35036/3652123151.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_probe = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 592,130 | Frozen parameters: 66,362,880\n",
      "\n",
      "ðŸ§  Training Head-Only (Frozen Encoder)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3790' max='3790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3790/3790 03:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.356524</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.842593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.342300</td>\n",
       "      <td>0.356477</td>\n",
       "      <td>0.839450</td>\n",
       "      <td>0.838337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluating Frozen Encoder Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3091977536678314, 'eval_accuracy': 0.8617668893838158, 'eval_f1': 0.8714621013392241, 'eval_runtime': 12.8979, 'eval_samples_per_second': 522.18, 'eval_steps_per_second': 8.218, 'epoch': 2.0}\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2648  396]\n",
      " [ 535 3156]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.870     0.850      3044\n",
      "           1      0.889     0.855     0.871      3691\n",
      "\n",
      "    accuracy                          0.862      6735\n",
      "   macro avg      0.860     0.862     0.861      6735\n",
      "weighted avg      0.863     0.862     0.862      6735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear probe training (frozen encoder, train head only)\n",
    "args = TrainingArguments(\n",
    "    output_dir=os.path.join(CONFIG[\"output_dir\"], \"probe\"),  # where to save checkpoints and logs\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],  # train batch size per device (GPU/CPU)\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],    # eval batch size per device (GPU/CPU)\n",
    "    learning_rate=CONFIG[\"learning_rate_headonly_finetune\"],            # optimizer learning rate for linear probe\n",
    "    num_train_epochs=CONFIG[\"epochs_headonly_finetune\"],                 # number of training epochs\n",
    "    weight_decay=CONFIG[\"weight_decay\"],                                 # L2 regularization strength\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],                                 # fraction of total steps for LR warmup\n",
    "    logging_steps=50,                                                    # log metrics every N training steps\n",
    "    eval_strategy=\"epoch\",                                         # run evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",                                               # save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,                                         # restore best checkpoint after training\n",
    "    seed=SEED,                                                           # RNG seed for reproducibility\n",
    "    report_to=\"none\",                                                     # disable external logging integrations\n",
    ")\n",
    "\n",
    "# create frozen encoder model\n",
    "frozen_model = build_model(encoder_requires_grad=False)\n",
    "# create trainer\n",
    "trainer_probe = Trainer(\n",
    "    model=frozen_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# Train the model\n",
    "print(\"\\nðŸ§  Training Head-Only (Frozen Encoder)...\")\n",
    "trainer_probe.train()\n",
    "\n",
    "# Test set evaluation -get evaluation metrics on test set via trainer.evaluate(your test set) \n",
    "# get raw test set logits via trainer.predict(your test set)\n",
    "# get the max of these logits (the predicted value)\n",
    "print(\"\\nðŸ“Š Evaluating Frozen Encoder Model...\")\n",
    "probe_metrics = trainer_probe.evaluate(tokenized_datasets[\"test\"])\n",
    "print(probe_metrics)\n",
    "\n",
    "# Get predictions for confusion matrix and classification report\n",
    "probs = trainer_probe.predict(tokenized_datasets[\"test\"])\n",
    "preds = np.argmax(probs.predictions, axis=-1)\n",
    "labels = probs.label_ids\n",
    "#print a confusion matrix and a Classification report (both imported above)\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(labels, preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e30c01",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (20 pts) **Full Fineâ€‘Tuning** (Encoder + Head)\n",
    "\n",
    "Now unfreeze the encoder and fineâ€‘tune endâ€‘toâ€‘end. We typically use a smaller learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27211d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 66,955,010 | Frozen parameters: 0\n",
      "\n",
      "ðŸ”¥ Training Full Fine-Tuned Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35036/2705351673.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_full = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3790' max='3790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3790/3790 11:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.236355</td>\n",
       "      <td>0.911697</td>\n",
       "      <td>0.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.309330</td>\n",
       "      <td>0.903670</td>\n",
       "      <td>0.906459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluating Full Fine-Tuned Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15776830911636353, 'eval_accuracy': 0.9446176688938381, 'eval_f1': 0.9488831026449226, 'eval_runtime': 12.9235, 'eval_samples_per_second': 521.145, 'eval_steps_per_second': 8.202, 'epoch': 2.0}\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2900  144]\n",
      " [ 229 3462]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.927     0.953     0.940      3044\n",
      "           1      0.960     0.938     0.949      3691\n",
      "\n",
      "    accuracy                          0.945      6735\n",
      "   macro avg      0.943     0.945     0.944      6735\n",
      "weighted avg      0.945     0.945     0.945      6735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#modify args as appropriate for full finetuning\n",
    "args_full = TrainingArguments(\n",
    "    output_dir=os.path.join(CONFIG[\"output_dir\"], \"full\"),\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    learning_rate=CONFIG[\"learning_rate_full_finetune\"],\n",
    "    num_train_epochs=CONFIG[\"epochs_full_finetune\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "# unfreeze all model layers (model is from previous step) for full finetuning\n",
    "full_model = build_model(frozen_model, encoder_requires_grad=True)\n",
    "# create trainer\n",
    "trainer_full = Trainer(\n",
    "    model=full_model,\n",
    "    args=args_full,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# Train the model\n",
    "print(\"\\nðŸ”¥ Training Full Fine-Tuned Model...\")\n",
    "trainer_full.train()\n",
    "# Test set evaluation -get evaluation metrics on test set via trainer.evaluate(your test set) \n",
    "# get raw test set logits via trainer.predict(your test set)\n",
    "# get the max of these logits (the predicted value)\n",
    "print(\"\\nðŸ“Š Evaluating Full Fine-Tuned Model...\")\n",
    "full_metrics = trainer_full.evaluate(tokenized_datasets[\"test\"])\n",
    "print(full_metrics)\n",
    "\n",
    "# Predictions and reports\n",
    "probs_full = trainer_full.predict(tokenized_datasets[\"test\"])\n",
    "preds_full = np.argmax(probs_full.predictions, axis=-1)\n",
    "labels_full = probs_full.label_ids\n",
    "#print a confusion matrix and a Classification report (both imported above)\n",
    "cm_full = confusion_matrix(labels_full, preds_full)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm_full)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(labels_full, preds_full, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43663e",
   "metadata": {},
   "source": [
    "\n",
    "## 8) (10 pts) Compare Results\n",
    "\n",
    "Let's quantify the improvement from full fineâ€‘tuning vs the frozen encoder baseline.\n",
    "Tell me what the delta is between the first model and the second for accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e6c377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (Frozen Encoder) Accuracy: 0.8618\n",
      "Full Fine-Tuned Accuracy:           0.9446\n",
      "Î” Accuracy: 0.0829  (9.61% relative improvement)\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "# Extract accuracy values from the two evaluation results\n",
    "acc_probe = probe_metrics[\"eval_accuracy\"]\n",
    "acc_full  = full_metrics[\"eval_accuracy\"]\n",
    "\n",
    "# Compute absolute and relative improvements\n",
    "delta_acc = acc_full - acc_probe\n",
    "rel_improvement = (delta_acc / acc_probe) * 100\n",
    "\n",
    "print(f\"Baseline (Frozen Encoder) Accuracy: {acc_probe:.4f}\")\n",
    "print(f\"Full Fine-Tuned Accuracy:           {acc_full:.4f}\")\n",
    "print(f\"Î” Accuracy: {delta_acc:.4f}  ({rel_improvement:.2f}% relative improvement)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd454234-3228-4ddf-b36c-17368a98764d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
