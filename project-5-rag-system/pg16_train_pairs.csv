query,positive_passage
What does this passage describe?,Internals 59.3. Foreign Data Wrapper Helper Functions .......................................................2424 59.4. Foreign Data Wrapper Query Planning .........................................................2425 59.5. Row Locking in Foreign Data Wrappers .......................................................2428 60. Writing a Table Sampling Method .........................................................................2430 60.1. Sampling Method Support Functions ............................................................2430 61. Writing a Custom Scan Provider ...........................................................................2433 61.1. Creating Custom Scan Paths .......................................................................2433 61.1.1. Custom Scan Path Callbacks ............................................................2434 61.2. Creating Custom Scan Plans .......................................................................2434 61.2.1. Custom Scan Plan Callbacks ............................................................2435 61.3. Executing Custom Scans ............................................................................2435 61.3.1. Custom Scan Execution Callbacks ....................................................2436 62. Genetic Query Optimizer ......................................................................................2438 62.1. Query Handling as a Complex Optimization Problem ......................................2438 62.2. Genetic Algorithms ...................................................................................2438 62.3. Genetic Query Optimization (GEQO) in PostgreSQL ......................................2439 62.3.1. Generating Possible Plans with GEQO ...............................................2440 62.3.2. Future Implementation Tasks for PostgreSQL GEQO ...........................2440 62.4. Further Reading .......................................................................................2441 63. Table Access Method Interface Definition ...............................................................2442 64. Index Access Method Interface Definition ...............................................................2443 64.1. Basic API Structure for Indexes ..................................................................2443 64.2. Index Access Method Functions ..................................................................2446 64.3. Index Scanning ........................................................................................2452 64.4. Index Locking Considerations .....................................................................2453 64.5. Index Uniqueness Checks ..........................................................................2454 64.6. Index Cost Estimation Functions .................................................................2455 65. Generic WAL Records .........................................................................................2459 66. Custom WAL Resource Managers .........................................................................2461 67. B-Tree Indexes ...................................................................................................2463 67.1. Introduction .............................................................................................2463 67.2. Behavior of B-Tree Operator Classes ...........................................................2463 67.3. B-Tree Support Functions ..........................................................................2464 67.4. Implementation ........................................................................................2467 67.4.1. B-Tree Structure ............................................................................2467 67.4.2. Bottom-up Index Deletion ...............................................................2467 67.4.3. Deduplication ................................................................................2468 68. GiST Indexes .....................................................................................................2470 68.1. Introduction .............................................................................................2470 68.2. Built-in Operator Classes ...........................................................................2470 68.3. Extensibility ............................................................................................2473 68.4. Implementation ........................................................................................2485 68.4.1. GiST Index Build Methods ..............................................................2485 68.5. Examples ................................................................................................2486 69. SP-GiST Indexes ................................................................................................2487 69.1. Introduction .............................................................................................2487 69.2.
What does this passage describe?,"sort order. Table 9.53. Array Operators Operator Description Example(s) anyarray @> anyarray → boolean Does the first array contain the second, that is, does each element appearing in the second array equal some element of the first array? (Duplicates are not treated specially, thus ARRAY[1] and ARRAY[1,1] are each considered to contain the other.) ARRAY[1,4,3] @> ARRAY[3,1,3] → t anyarray <@ anyarray → boolean Is the first array contained by the second? ARRAY[2,2,7] <@ ARRAY[1,7,4,2,6] → t anyarray && anyarray → boolean Do the arrays overlap, that is, have any elements in common? ARRAY[1,4,3] && ARRAY[2,1] → t anycompatiblearray || anycompatiblearray → anycompatiblearray Concatenates the two arrays. Concatenating a null or empty array is a no-op; otherwise the arrays must have the same number of dimensions (as illustrated by the first example) or differ in number of dimensions by one (as illustrated by the second). If the arrays are not of identical element types, they will be coerced to a common type (see Section 10.5 ). ARRAY[1,2,3] || ARRAY[4,5,6,7] → {1,2,3,4,5,6,7} ARRAY[1,2,3] || ARRAY[[4,5,6],[7,8,9.9]] → {{1,2,3},{4,5,6}, {7,8,9.9}} anycompatible || anycompatiblearray → anycompatiblearray Concatenates an element onto the front of an array (which must be empty or one-dimen- sional). 346"
What does this passage describe?,"SQL Syntax more expressions (separated by commas) for the row field values, and finally a right parenthesis. For example: SELECT ROW(1,2.5,'this is a test'); The key word ROW is optional when there is more than one expression in the list. A row constructor can include the syntax rowvalue .*, which will be expanded to a list of the elements of the row value, just as occurs when the .* syntax is used at the top level of a SELECT list (see Section 8.16.5 ). For example, if table t has columns f1 and f2, these are the same: SELECT ROW(t.*, 42) FROM t; SELECT ROW(t.f1, t.f2, 42) FROM t; Note Before PostgreSQL 8.2, the .* syntax was not expanded in row constructors, so that writing ROW(t.*, 42) created a two-field row whose first field was another row value. The new behavior is usually more useful. If you need the old behavior of nested row values, write the inner row value without .*, for instance ROW(t, 42) . By default, the value created by a ROW expression is of an anonymous record type. If necessary, it can be cast to a named composite type — either the row type of a table, or a composite type created with CREATE TYPE AS . An explicit cast might be needed to avoid ambiguity. For example: CREATE TABLE mytable(f1 int, f2 float, f3 text); CREATE FUNCTION getf1(mytable) RETURNS int AS 'SELECT $1.f1' LANGUAGE SQL; -- No cast needed since only one getf1() exists SELECT"
What does this passage describe?,"SQL Key Words Key Word PostgreSQL SQL:2023 SQL:2016 SQL-92 TIMESTAMP non-reserved (can- not be function or type)reserved reserved reserved TIMEZONE_HOUR reserved reserved reserved TIMEZONE_MINUTE reserved reserved reserved TO reserved, requires ASreserved reserved reserved TOKEN non-reserved non-reserved TOP_LEVEL_COUNT non-reserved non-reserved TRAILING reserved reserved reserved reserved TRANSACTION non-reserved non-reserved non-reserved reserved TRANSACTIONS_COMMITTED non-reserved non-reserved TRANSACTIONS_ROLLED_ BACK non-reserved non-reserved TRANSACTION_ACTIVE non-reserved non-reserved TRANSFORM non-reserved non-reserved non-reserved TRANSFORMS non-reserved non-reserved TRANSLATE reserved reserved reserved TRANSLATE_REGEX reserved reserved TRANSLATION reserved reserved reserved TREAT non-reserved (can- not be function or type)reserved reserved TRIGGER non-reserved reserved reserved TRIGGER_CATALOG non-reserved non-reserved TRIGGER_NAME non-reserved non-reserved TRIGGER_SCHEMA non-reserved non-reserved TRIM non-reserved (can- not be function or type)reserved reserved reserved TRIM_ARRAY reserved reserved TRUE reserved reserved reserved reserved TRUNCATE non-reserved reserved reserved TRUSTED non-reserved TYPE non-reserved non-reserved non-reserved non-reserved TYPES non-reserved UESCAPE non-reserved reserved reserved UNBOUNDED non-reserved non-reserved non-reserved UNCOMMITTED non-reserved non-reserved non-reserved non-reserved UNCONDITIONAL non-reserved non-reserved UNDER non-reserved non-reserved UNENCRYPTED non-reserved 2600"
What does this passage describe?,"Monitoring Database Activity Whenever VACUUM is running, the pg_stat_progress_vacuum view will contain one row for each backend (including autovacuum worker processes) that is currently vacuuming. The tables be- low describe the information that will be reported and provide information about how to interpret it. Progress for VACUUM FULL commands is reported via pg_stat_progress_cluster because both VACUUM FULL and CLUSTER rewrite the table, while regular VACUUM only modifies it in place. See Section 28.4.2 . Table 28.44. pg_stat_progress_vacuum View Column Type Description pid integer Process ID of backend. datid oid OID of the database to which this backend is connected. datname name Name of the database to which this backend is connected. relid oid OID of the table being vacuumed. phase text Current processing phase of vacuum. See Table 28.45 . heap_blks_total bigint Total number of heap blocks in the table. This number is reported as of the beginning of the scan; blocks added later will not be (and need not be) visited by this VACUUM. heap_blks_scanned bigint Number of heap blocks scanned. Because the visibility map is used to optimize scans, some blocks will be skipped without inspection; skipped blocks are included in this total, so that this number will eventually become equal to heap_blks_total when the vac- uum is complete. This counter only advances when the phase is scanning heap . heap_blks_vacuumed bigint Number of heap blocks vacuumed. Unless the table has no indexes, this counter only ad- vances when the phase is vacuuming heap . Blocks that contain"
What does this passage describe?,"to date. 25.1.4. Updating the Visibility Map Vacuum maintains a visibility map for each table to keep track of which pages contain only tuples that are known to be visible to all active transactions (and all future transactions, until the page is again modified). This has two purposes. First, vacuum itself can skip such pages on the next run, since there is nothing to clean up. Second, it allows PostgreSQL to answer some queries using only the index, without reference to the underlying table. Since PostgreSQL indexes don't contain tuple visibility information, a normal index scan fetches the heap tuple for each matching index entry, to check whether it should be seen by the current transaction. An index-only scan , on the other hand, checks the visibility map first. If it's known that all tuples on the page are visible, the heap fetch can be skipped. This is most useful on large data sets where the visibility map can prevent disk accesses. The visibility map is vastly smaller than the heap, so it can easily be cached even when the heap is very large. 25.1.5. Preventing Transaction ID Wraparound Failures PostgreSQL's MVCC transaction semantics depend on being able to compare transaction ID (XID) numbers: a row version with an insertion XID greater than the current transaction's XID is “in the future” and should not be visible to the current transaction. But since transaction IDs have limited size (32 bits) a cluster that runs for a long time (more than 4"
What does this passage describe?,"The valid options are data for main data files, wal for WAL files, and wal_init for WAL files when being initially allocated. This parameter can only be set at server start. Some operating systems and file systems do not support direct I/O, so non-default settings may be rejected at startup or cause errors. Currently this feature reduces performance, and is intended for developer testing only. debug_parallel_query (enum) Allows the use of parallel queries for testing purposes even in cases where no performance benefit is expected. The allowed values of debug_parallel_query are off (use parallel mode only when it is expected to improve performance), on (force parallel query for all queries for which it 680"
What does this passage describe?,"Indexes SELECT * FROM people WHERE (first_name || ' ' || last_name) = 'John Smith'; then it might be worth creating an index like this: CREATE INDEX people_names ON people ((first_name || ' ' || last_name)); The syntax of the CREATE INDEX command normally requires writing parentheses around index expressions, as shown in the second example. The parentheses can be omitted when the expression is just a function call, as in the first example. Index expressions are relatively expensive to maintain, because the derived expression(s) must be computed for each row insertion and non-HOT update. However, the index expressions are not recom- puted during an indexed search, since they are already stored in the index. In both examples above, the system sees the query as just WHERE indexedcolumn = 'constant' and so the speed of the search is equivalent to any other simple index query. Thus, indexes on expressions are useful when retrieval speed is more important than insertion and update speed. 11.8. Partial Indexes A partial index is an index built over a subset of a table; the subset is defined by a conditional expres- sion (called the predicate of the partial index). The index contains entries only for those table rows that satisfy the predicate. Partial indexes are a specialized feature, but there are several situations in which they are useful. One major reason for using a partial index is to avoid indexing common values. Since a query searching for a common value (one that accounts for more"
What does this passage describe?,"Date/Time Support 5.If BC was not specified, and if the year field was two digits in length, then adjust the year to four digits. If the field is less than 70, then add 2000, otherwise add 1900. Tip Gregorian years AD 1–99 can be entered by using 4 digits with leading zeros (e.g., 0099 is AD 99). B.2. Handling of Invalid or Ambiguous Time- stamps Ordinarily, if a date/time string is syntactically valid but contains out-of-range field values, an error will be thrown. For example, input specifying the 31st of February will be rejected. During a daylight-savings-time transition, it is possible for a seemingly valid timestamp string to represent a nonexistent or ambiguous timestamp. Such cases are not rejected; the ambiguity is resolved by determining which UTC offset to apply. For example, supposing that the TimeZone parameter is set to America/New_York , consider => SELECT '2018-03-11 02:30'::timestamptz; timestamptz ------------------------ 2018-03-11 03:30:00-04 (1 row) Because that day was a spring-forward transition date in that time zone, there was no civil time instant 2:30AM; clocks jumped forward from 2AM EST to 3AM EDT. PostgreSQL interprets the given time as if it were standard time (UTC-5), which then renders as 3:30AM EDT (UTC-4). Conversely, consider the behavior during a fall-back transition: => SELECT '2018-11-04 01:30'::timestamptz; timestamptz ------------------------ 2018-11-04 01:30:00-05 (1 row) On that date, there were two possible interpretations of 1:30AM; there was 1:30AM EDT, and then an hour later after clocks jumped back from 2AM EDT to 1AM EST, there was 1:30AM"
What does this passage describe?,"which consists of exactly 2 elements). The first element of each array entry is the namespace name (alias), the second the namespace URI. It is not required that aliases provided in this array be the same as those being used in the XML document itself (in other words, both in the XML document and in the xpath function context, aliases are local). Example: SELECT xpath('/my:a/text()', '<my:a xmlns:my=""http:// example.com"">test</my:a>', ARRAY[ARRAY['my', 'http://example.com']]); xpath -------- {test} 314"
What does this passage describe?,"Frontend/Backend Protocol Int32 Number of protocol options not recognized by the server. Then, for protocol option not recognized by the server, there is the following: String The option name. NoData (B) Byte1('n') Identifies the message as a no-data indicator. Int32(4) Length of message contents in bytes, including self. NoticeResponse (B) Byte1('N') Identifies the message as a notice. Int32 Length of message contents in bytes, including self. The message body consists of one or more identified fields, followed by a zero byte as a termi- nator. Fields can appear in any order. For each field there is the following: Byte1 A code identifying the field type; if zero, this is the message terminator and no string follows. The presently defined field types are listed in Section 55.8 . Since more field types might be added in future, frontends should silently ignore fields of unrecognized type. String The field value. NotificationResponse (B) Byte1('A') Identifies the message as a notification response. Int32 Length of message contents in bytes, including self. Int32 The process ID of the notifying backend process. String The name of the channel that the notify has been raised on. String The “payload” string passed from the notifying process. 2372"
What does this passage describe?,"SQL Key Words Key Word PostgreSQL SQL:2023 SQL:2016 SQL-92 NUMERIC non-reserved (can- not be function or type)reserved reserved reserved OBJECT non-reserved non-reserved non-reserved OCCURRENCE non-reserved non-reserved OCCURRENCES_REGEX reserved reserved OCTETS non-reserved non-reserved OCTET_LENGTH reserved reserved reserved OF non-reserved reserved reserved reserved OFF non-reserved non-reserved non-reserved OFFSET reserved, requires ASreserved reserved OIDS non-reserved OLD non-reserved reserved reserved OMIT reserved reserved ON reserved, requires ASreserved reserved reserved ONE reserved reserved ONLY reserved reserved reserved reserved OPEN reserved reserved reserved OPERATOR non-reserved OPTION non-reserved non-reserved non-reserved reserved OPTIONS non-reserved non-reserved non-reserved OR reserved reserved reserved reserved ORDER reserved, requires ASreserved reserved reserved ORDERING non-reserved non-reserved ORDINALITY non-reserved non-reserved non-reserved OTHERS non-reserved non-reserved non-reserved OUT non-reserved (can- not be function or type)reserved reserved OUTER reserved (can be function or type)reserved reserved reserved OUTPUT non-reserved non-reserved reserved OVER non-reserved, re- quires ASreserved reserved OVERFLOW non-reserved non-reserved OVERLAPS reserved (can be function or type), requires ASreserved reserved reserved OVERLAY non-reserved (can- not be function or type)reserved reserved 2593"
What does this passage describe?,"DROP TEXT SEARCH PARSER DROP TEXT SEARCH PARSER — remove a text search parser Synopsis DROP TEXT SEARCH PARSER [ IF EXISTS ] name [ CASCADE | RESTRICT ] Description DROP TEXT SEARCH PARSER drops an existing text search parser. You must be a superuser to use this command. Parameters IF EXISTS Do not throw an error if the text search parser does not exist. A notice is issued in this case. name The name (optionally schema-qualified) of an existing text search parser. CASCADE Automatically drop objects that depend on the text search parser, and in turn all objects that depend on those objects (see Section 5.14 ). RESTRICT Refuse to drop the text search parser if any objects depend on it. This is the default. Examples Remove the text search parser my_parser : DROP TEXT SEARCH PARSER my_parser; This command will not succeed if there are any existing text search configurations that use the parser. Add CASCADE to drop such configurations along with the parser. Compatibility There is no DROP TEXT SEARCH PARSER statement in the SQL standard. See Also ALTER TEXT SEARCH PARSER , CREATE TEXT SEARCH PARSER 1889"
What does this passage describe?,"a bracket expression, the name of a character class enclosed in [: and :] stands for the list of all characters belonging to that class. A character class cannot be used as an endpoint of a range. The POSIX standard defines these character class names: alnum (letters and numeric digits), alpha (let- ters), blank (space and tab), cntrl (control characters), digit (numeric digits), graph (printable characters except space), lower (lower-case letters), print (printable characters including space), punct (punctuation), space (any white space), upper (upper-case letters), and xdigit (hexadec- imal digits). The behavior of these standard character classes is generally consistent across platforms for characters in the 7-bit ASCII set. Whether a given non-ASCII character is considered to belong to one of these classes depends on the collation that is used for the regular-expression function or oper- ator (see Section 24.2 ), or by default on the database's LC_CTYPE locale setting (see Section 24.1 ). The classification of non-ASCII characters can vary across platforms even in similarly-named locales. (But the C locale never considers any non-ASCII characters to belong to any of these classes.) In addition to these standard character classes, PostgreSQL defines the word character class, which is the same as alnum plus the underscore ( _) character, and the ascii character class, which contains exactly the 7-bit ASCII set. There are two special cases of bracket expressions: the bracket expressions [[:<:]] and [[:>:]] are constraints, matching empty strings at the beginning and end of a word respectively. A word"
What does this passage describe?,"pgbench Caution pgbench -i creates four tables pgbench_accounts , pgbench_branches , pg- bench_history , and pgbench_tellers , destroying any existing tables of these names. Be very careful to use another database if you have tables having these names! At the default “scale factor” of 1, the tables initially contain this many rows: table # of rows --------------------------------- pgbench_branches 1 pgbench_tellers 10 pgbench_accounts 100000 pgbench_history 0 You can (and, for most purposes, probably should) increase the number of rows by using the -s (scale factor) option. The -F (fillfactor) option might also be used at this point. Once you have done the necessary setup, you can run your benchmark with a command that doesn't include -i, that is pgbench [ options ] dbname In nearly all cases, you'll need some options to make a useful test. The most important options are - c (number of clients), -t (number of transactions), -T (time limit), and -f (specify a custom script file). See below for a full list. Options The following is divided into three subsections. Different options are used during database initializa- tion and while running benchmarks, but some options are useful in both cases. Initialization Options pgbench accepts the following command-line initialization arguments: dbname Specifies the name of the database to test in. If this is not specified, the environment variable PGDATABASE is used. If that is not set, the user name specified for the connection is used. -i --initialize Required to invoke initialization mode. -I init_steps --init-steps=init_steps Perform just a"
What does this passage describe?,it already exists 1371
What does this passage describe?,"a table. TRIGGER Allows creation of a trigger on a table, view, etc. CREATE For databases, allows new schemas and publications to be created within the database, and allows trusted extensions to be installed within the database. 76"
What does this passage describe?,"SET DEFAULT can take a column list to specify which columns to set. Normally, all columns of the foreign-key constraint are set; setting only a subset is useful in some special cases. Consider the following example: CREATE TABLE tenants ( tenant_id integer PRIMARY KEY ); CREATE TABLE users ( tenant_id integer REFERENCES tenants ON DELETE CASCADE, user_id integer NOT NULL, PRIMARY KEY (tenant_id, user_id) ); CREATE TABLE posts ( tenant_id integer REFERENCES tenants ON DELETE CASCADE, 70"
What does this passage describe?,"Functions and Operators When adding an interval value to (or subtracting an interval value from) a timestamp or timestamp with time zone value, the months, days, and microseconds fields of the inter- val value are handled in turn. First, a nonzero months field advances or decrements the date of the timestamp by the indicated number of months, keeping the day of month the same unless it would be past the end of the new month, in which case the last day of that month is used. (For example, March 31 plus 1 month becomes April 30, but March 31 plus 2 months becomes May 31.) Then the days field advances or decrements the date of the timestamp by the indicated number of days. In both these steps the local time of day is kept the same. Finally, if there is a nonzero microseconds field, it is added or subtracted literally. When doing arithmetic on a timestamp with time zone value in a time zone that recognizes DST, this means that adding or subtracting (say) interval '1 day' does not necessarily have the same result as adding or subtracting interval '24 hours' . For example, with the session time zone set to America/Denver : SELECT timestamp with time zone '2005-04-02 12:00:00-07' + interval '1 day'; Result: 2005-04-03 12:00:00-06 SELECT timestamp with time zone '2005-04-02 12:00:00-07' + interval '24 hours'; Result: 2005-04-03 13:00:00-06 This happens because an hour was skipped due to a change in daylight saving time at 2005-04-03 02:00:00 in"
What does this passage describe?,"Server Configuration vacuum_freeze_table_age (integer ) VACUUM performs an aggressive scan if the table's pg_class .relfrozenxid field has reached the age specified by this setting. An aggressive scan differs from a regular VACUUM in that it visits every page that might contain unfrozen XIDs or MXIDs, not just those that might contain dead tuples. The default is 150 million transactions. Although users can set this value anywhere from zero to two billion, VACUUM will silently limit the effective value to 95% of au- tovacuum_freeze_max_age , so that a periodic manual VACUUM has a chance to run before an anti-wraparound autovacuum is launched for the table. For more information see Section 25.1.5 . vacuum_freeze_min_age (integer ) Specifies the cutoff age (in transactions) that VACUUM should use to decide whether to trigger freezing of pages that have an older XID. The default is 50 million transactions. Although users can set this value anywhere from zero to one billion, VACUUM will silently limit the effective value to half the value of autovacuum_freeze_max_age , so that there is not an unreasonably short time between forced autovacuums. For more information see Section 25.1.5 . vacuum_failsafe_age (integer ) Specifies the maximum age (in transactions) that a table's pg_class .relfrozenxid field can attain before VACUUM takes extraordinary measures to avoid system-wide transaction ID wraparound failure. This is VACUUM's strategy of last resort. The failsafe typically triggers when an autovacuum to prevent transaction ID wraparound has already been running for some time, though it's possible for the failsafe to"
What does this passage describe?,"Database Roles Every connection to the database server is made using the name of some particular role, and this role determines the initial access privileges for commands issued in that connection. The role name to use for a particular database connection is indicated by the client that is initiating the connection request in an application-specific fashion. For example, the psql program uses the -U command line option to indicate the role to connect as. Many applications assume the name of the current operating system user by default (including createuser and psql). Therefore it is often convenient to maintain a naming correspondence between roles and operating system users. The set of database roles a given client connection can connect as is determined by the client authen- tication setup, as explained in Chapter 21 . (Thus, a client is not limited to connect as the role matching its operating system user, just as a person's login name need not match his or her real name.) Since the role identity determines the set of privileges available to a connected client, it is important to carefully configure privileges when setting up a multiuser environment. 22.2. Role Attributes A database role can have a number of attributes that define its privileges and interact with the client authentication system. login privilege Only roles that have the LOGIN attribute can be used as the initial role name for a database connection. A role with the LOGIN attribute can be considered the same as a “database user” ."
What does this passage describe?,"CREATE OPERATOR The obsolete options SORT1, SORT2, LTCMP, and GTCMP were formerly used to specify the names of sort operators associated with a merge-joinable operator. This is no longer necessary, since information about associated operators is found by looking at B-tree operator families instead. If one of these options is given, it is ignored except for implicitly setting MERGES true. Use DROP OPERATOR to delete user-defined operators from a database. Use ALTER OPERATOR to modify operators in a database. Examples The following command defines a new operator, area-equality, for the data type box: CREATE OPERATOR === ( LEFTARG = box, RIGHTARG = box, FUNCTION = area_equal_function, COMMUTATOR = ===, NEGATOR = !==, RESTRICT = area_restriction_function, JOIN = area_join_function, HASHES, MERGES ); Compatibility CREATE OPERATOR is a PostgreSQL extension. There are no provisions for user-defined operators in the SQL standard. See Also ALTER OPERATOR , CREATE OPERATOR CLASS , DROP OPERATOR 1728"
What does this passage describe?,"pg_dumpall -f filename --file=filename Send output to the specified file. If this is omitted, the standard output is used. -g --globals-only Dump only global objects (roles and tablespaces), no databases. -O --no-owner Do not output commands to set ownership of objects to match the original database. By default, pg_dumpall issues ALTER OWNER or SET SESSION AUTHORIZATION statements to set ownership of created schema elements. These statements will fail when the script is run unless it is started by a superuser (or the same user that owns all of the objects in the script). To make a script that can be restored by any user, but will give that user ownership of all the objects, specify -O. -r --roles-only Dump only roles, no databases or tablespaces. -s --schema-only Dump only the object definitions (schema), not data. -S username --superuser=username Specify the superuser user name to use when disabling triggers. This is relevant only if --dis- able-triggers is used. (Usually, it's better to leave this out, and instead start the resulting script as superuser.) -t --tablespaces-only Dump only tablespaces, no databases or roles. -v --verbose Specifies verbose mode. This will cause pg_dumpall to output start/stop times to the dump file, and progress messages to standard error. Repeating the option causes additional debug-level mes- sages to appear on standard error. The option is also passed down to pg_dump. -V --version Print the pg_dumpall version and exit. -x --no-privileges --no-acl Prevent dumping of access privileges (grant/revoke commands). --binary-upgrade This option is for use by"
What does this passage describe?,"are integer for whole numbers, numeric for possibly fractional numbers, text for character strings, date for dates, time for time-of-day values, and timestamp for values containing both date and time. To create a table, you use the aptly named CREATE TABLE command. In this command you specify at least a name for the new table, the names of the columns and the data type of each column. For example: CREATE TABLE my_first_table ( first_column text, second_column integer ); This creates a table named my_first_table with two columns. The first column is named first_column and has a data type of text; the second column has the name second_column and the type integer . The table and column names follow the identifier syntax explained in Sec- tion 4.1.1 . The type names are usually also identifiers, but there are some exceptions. Note that the column list is comma-separated and surrounded by parentheses. Of course, the previous example was heavily contrived. Normally, you would give names to your tables and columns that convey what kind of data they store. So let's look at a more realistic example: CREATE TABLE products ( product_no integer, name text, 59"
What does this passage describe?,"REINDEX the index is being rebuilt, this method is useful for rebuilding indexes in a production environment. Of course, the extra CPU, memory and I/O load imposed by the index rebuild may slow down other operations. The following steps occur in a concurrent reindex. Each step is run in a separate transaction. If there are multiple indexes to be rebuilt, then each step loops through all the indexes before moving to the next step. 1.A new transient index definition is added to the catalog pg_index . This definition will be used to replace the old index. A SHARE UPDATE EXCLUSIVE lock at session level is taken on the indexes being reindexed as well as their associated tables to prevent any schema modification while processing. 2.A first pass to build the index is done for each new index. Once the index is built, its flag pg_in- dex.indisready is switched to “true” to make it ready for inserts, making it visible to other sessions once the transaction that performed the build is finished. This step is done in a separate transaction for each index. 3.Then a second pass is performed to add tuples that were added while the first pass was running. This step is also done in a separate transaction for each index. 4.All the constraints that refer to the index are changed to refer to the new index definition, and the names of the indexes are changed. At this point, pg_index.indisvalid is switched to “true” for the new index and"
What does this passage describe?,"Server Configuration hash_mem_multiplier (floating point ) Used to compute the maximum amount of memory that hash-based operations can use. The final limit is determined by multiplying work_mem by hash_mem_multiplier . The default value is 2.0, which makes hash-based operations use twice the usual work_mem base amount. Consider increasing hash_mem_multiplier in environments where spilling by query oper- ations is a regular occurrence, especially when simply increasing work_mem results in memory pressure (memory pressure typically takes the form of intermittent out of memory errors). The default setting of 2.0 is often effective with mixed workloads. Higher settings in the range of 2.0 - 8.0 or more may be effective in environments where work_mem has already been increased to 40MB or more. maintenance_work_mem (integer ) Specifies the maximum amount of memory to be used by maintenance operations, such as VAC- UUM, CREATE INDEX , and ALTER TABLE ADD FOREIGN KEY . If this value is specified without units, it is taken as kilobytes. It defaults to 64 megabytes ( 64MB). Since only one of these operations can be executed at a time by a database session, and an installation normally doesn't have many of them running concurrently, it's safe to set this value significantly larger than work_mem . Larger settings might improve performance for vacuuming and for restoring database dumps. Note that when autovacuum runs, up to autovacuum_max_workers times this memory may be allocated, so be careful not to set the default value too high. It may be useful to control for this"
What does this passage describe?,"inner_consistent function should return either all or none of the nodes as targets for continuing the index search, since they are all equivalent. This may or may not require any special-case code, depending on how much the inner_consistent function normally assumes about the meaning of the nodes. 69.5. Examples The PostgreSQL source distribution includes several examples of index operator classes for SP- GiST, as described in Table 69.1 . Look into src/backend/access/spgist/ and src/back- end/utils/adt/ to see the code. 2499"
What does this passage describe?,"zero) data bytea Actual data stored in the large object. This will never be more than LOBLKSIZE bytes and might be less. Each row of pg_largeobject holds data for one page of a large object, beginning at byte offset (pageno * LOBLKSIZE ) within the object. The implementation allows sparse storage: pages might be missing, and might be shorter than LOBLKSIZE bytes even if they are not the last page of the object. Missing regions within a large object read as zeroes. 53.31. pg_largeobject_metadata The catalog pg_largeobject_metadata holds metadata associated with large objects. The ac- tual large object data is stored in pg_largeobject . Table 53.31. pg_largeobject_metadata Columns Column Type Description oid oid Row identifier lomowner oid (references pg_authid .oid) Owner of the large object lomacl aclitem[] Access privileges; see Section 5.7 for details 2278"
What does this passage describe?,is not necessary to implement this function for access methods which do not support parallel scans or in cases where the shared memory space required needs no initialization. void 2451
What does this passage describe?,"DROP TABLESPACE DROP TABLESPACE — remove a tablespace Synopsis DROP TABLESPACE [ IF EXISTS ] name Description DROP TABLESPACE removes a tablespace from the system. A tablespace can only be dropped by its owner or a superuser. The tablespace must be empty of all database objects before it can be dropped. It is possible that objects in other databases might still reside in the tablespace even if no objects in the current database are using the tablespace. Also, if the tablespace is listed in the temp_tablespaces setting of any active session, the DROP might fail due to temporary files residing in the tablespace. Parameters IF EXISTS Do not throw an error if the tablespace does not exist. A notice is issued in this case. name The name of a tablespace. Notes DROP TABLESPACE cannot be executed inside a transaction block. Examples To remove tablespace mystuff from the system: DROP TABLESPACE mystuff; Compatibility DROP TABLESPACE is a PostgreSQL extension. See Also CREATE TABLESPACE , ALTER TABLESPACE 1886"
What does this passage describe?,"PL/pgSQL — SQL Procedural Language The mutable nature of record variables presents another problem in this connection. When fields of a record variable are used in expressions or statements, the data types of the fields must not change from one call of the function to the next, since each expression will be analyzed using the data type that is present when the expression is first reached. EXECUTE can be used to get around this problem when necessary. If the same function is used as a trigger for more than one table, PL/pgSQL prepares and caches statements independently for each such table — that is, there is a cache for each trigger function and table combination, not just for each function. This alleviates some of the problems with varying data types; for instance, a trigger function will be able to work successfully with a column named key even if it happens to have different types in different tables. Likewise, functions having polymorphic argument types have a separate statement cache for each combination of actual argument types they have been invoked for, so that data type differences do not cause unexpected failures. Statement caching can sometimes have surprising effects on the interpretation of time-sensitive values. For example there is a difference between what these two functions do: CREATE FUNCTION logfunc1(logtxt text) RETURNS void AS $$ BEGIN INSERT INTO logtable VALUES (logtxt, 'now'); END; $$ LANGUAGE plpgsql; and: CREATE FUNCTION logfunc2(logtxt text) RETURNS void AS $$ DECLARE curtime timestamp; BEGIN curtime := 'now';"
What does this passage describe?,"However, unless a module is specifically designed to be used in this way by non-superusers, this is usually not the right setting to use. Look at session_preload_libraries instead. session_preload_libraries (string) This variable specifies one or more shared libraries that are to be preloaded at connection start. It contains a comma-separated list of library names, where each name is interpreted as for the LOAD command. Whitespace between entries is ignored; surround a library name with double quotes if you need to include whitespace or commas in the name. The parameter value only takes effect at the start of the connection. Subsequent changes have no effect. If a specified library is not found, the connection attempt will fail. Only superusers and users with the appropriate SET privilege can change this setting. The intent of this feature is to allow debugging or performance-measurement libraries to be loaded into specific sessions without an explicit LOAD command being given. For example, auto_explain could be enabled for all sessions under a given user name by setting this parameter with ALTER ROLE SET . Also, this parameter can be changed without restarting the server (but changes only 672"
What does this passage describe?,"Logical Decoding The parameters are analogous to the change_cb callback. However, because TRUNCATE actions on tables connected by foreign keys need to be executed together, this callback receives an array of relations instead of just a single one. See the description of the TRUNCATE statement for details. 49.6.4.7. Origin Filter Callback The optional filter_by_origin_cb callback is called to determine whether data that has been replayed from origin_id is of interest to the output plugin. typedef bool (*LogicalDecodeFilterByOriginCB) (struct LogicalDecodingContext *ctx, RepOriginId origin_id); The ctx parameter has the same contents as for the other callbacks. No information but the origin is available. To signal that changes originating on the passed in node are irrelevant, return true, causing them to be filtered away; false otherwise. The other callbacks will not be called for transactions and changes that have been filtered away. This is useful when implementing cascading or multidirectional replication solutions. Filtering by the origin allows to prevent replicating the same changes back and forth in such setups. While transactions and changes also carry information about the origin, filtering via this callback is noticeably more efficient. 49.6.4.8. Generic Message Callback The optional message_cb callback is called whenever a logical decoding message has been decoded. typedef void (*LogicalDecodeMessageCB) (struct LogicalDecodingContext *ctx, ReorderBufferTXN *txn, XLogRecPtr message_lsn, bool transactional, const char *prefix, Size message_size, const char *message); The txn parameter contains meta information about the transaction, like the time stamp at which it has been committed and its XID. Note however that it can be"
What does this passage describe?,"used to check the integrity of a backup against the backup manifest. --manifest-force-encode Forces all filenames in the backup manifest to be hex-encoded. If this option is not specified, only non-UTF8 filenames are hex-encoded. This option is mostly intended to test that tools which read a backup manifest file properly handle this case. --no-estimate-size Prevents the server from estimating the total amount of backup data that will be streamed, result- ing in the backup_total column in the pg_stat_progress_basebackup view always being NULL. Without this option, the backup will start by enumerating the size of the entire database, and then go back and send the actual contents. This may make the backup take slightly longer, and in particular it will take longer before the first data is sent. This option is useful to avoid such estimation time if it's too long. This option is not allowed when using --progress . --no-manifest Disables generation of a backup manifest. If this option is not specified, the server will generate and send a backup manifest which can be verified using pg_verifybackup . The manifest is a list of every file present in the backup with the exception of any WAL files that may be included. It also stores the size, last modification time, and an optional checksum for each file. --no-slot Prevents the creation of a temporary replication slot for the backup. By default, if log streaming is selected but no slot name is given with the -S option, then a temporary replication slot"
What does this passage describe?,"you can issue queries to see the disk usage of any table: SELECT pg_relation_filepath(oid), relpages FROM pg_class WHERE relname = 'customer'; pg_relation_filepath | relpages ----------------------+---------- base/16384/16806 | 60 (1 row) Each page is typically 8 kilobytes. (Remember, relpages is only updated by VACUUM, ANALYZE , and a few DDL commands such as CREATE INDEX .) The file path name is of interest if you want to examine the table's disk file directly. To show the space used by TOAST tables, use a query like the following: SELECT relname, relpages FROM pg_class, (SELECT reltoastrelid FROM pg_class WHERE relname = 'customer') AS ss WHERE oid = ss.reltoastrelid OR oid = (SELECT indexrelid FROM pg_index WHERE indrelid = ss.reltoastrelid) ORDER BY relname; relname | relpages ----------------------+---------- pg_toast_16806 | 0 pg_toast_16806_index | 1 You can easily display index sizes, too: SELECT c2.relname, c2.relpages 853"
What does this passage describe?,"appear in a WHERE clause for a foreign table. Thus, a built-in function that was added since the remote server's release might be sent to it for execution, resulting in “function does not exist” or a similar error. This type of failure can be worked around by rewriting the query, for example by embedding the foreign table reference in a sub- SELECT with OFFSET 0 as an optimization fence, and placing the problematic function or operator outside the sub- SELECT. F.38.8. Configuration Parameters postgres_fdw.application_name (string) Specifies a value for application_name configuration parameter used when postgres_fdw es- tablishes a connection to a foreign server. This overrides application_name option of the server object. Note that change of this parameter doesn't affect any existing connections until they are re-established. postgres_fdw.application_name can be any string of any length and contain even non- ASCII characters. However when it's passed to and used as application_name in a foreign server, note that it will be truncated to less than NAMEDATALEN characters. Anything other than printable ASCII characters are replaced with C-style hexadecimal escapes . See application_name for details. % characters begin “escape sequences” that are replaced with status information as outlined below. Unrecognized escapes are ignored. Other characters are copied straight to the application name. Note that it's not allowed to specify a plus/minus sign or a numeric literal after the % and before the option, for alignment and padding. Escape Effect %a Application name on local server %c Session ID on local server (see log_line_prefix for"
What does this passage describe?,Example Program ..........................................................1002 36.1. Example SQLDA Program .................................................................................1055 36.2. ECPG Program Accessing Large Objects ..............................................................1069 42.1. Manual Installation of PL/Perl ............................................................................1306 43.1. Quoting Values in Dynamic Queries ....................................................................1323 43.2. Exceptions with UPDATE/INSERT ......................................................................1338 43.3. A PL/pgSQL Trigger Function ............................................................................1352 43.4. A PL/pgSQL Trigger Function for Auditing ..........................................................1353 43.5. A PL/pgSQL View Trigger Function for Auditing ..................................................1354 43.6. A PL/pgSQL Trigger Function for Maintaining a Summary Table .............................1355 43.7. Auditing with Transition Tables ..........................................................................1357 43.8. A PL/pgSQL Event Trigger Function ...................................................................1359 43.9. Porting a Simple Function from PL/SQL to PL/pgSQL ............................................1367 43.10. Porting a Function that Creates Another Function from PL/SQL to PL/pgSQL ............1368 43.11. Porting a Procedure With String Manipulation and OUT Parameters from PL/SQL to PL/pgSQL ...............................................................................................................1369 43.12. Porting a Procedure from PL/SQL to PL/pgSQL ..................................................1371 F.1. Create a Foreign Table for PostgreSQL CSV Logs ...................................................2770 xxxi
What does this passage describe?,"Release Notes E.9.1. Migration to Version 16.3 A dump/restore is not required for those running 16.X. However, a security vulnerability was found in the system views pg_stats_ext and pg_stat- s_ext_exprs , potentially allowing authenticated database users to see data they shouldn't. If this is of concern in your installation, follow the steps in the first changelog entry below to rectify it. Also, if you are upgrading from a version earlier than 16.2, see Section E.10 . E.9.2. Changes •Restrict visibility of pg_stats_ext and pg_stats_ext_exprs entries to the table owner (Nathan Bossart) These views failed to hide statistics for expressions that involve columns the accessing user does not have permission to read. View columns such as most_common_vals might expose securi- ty-relevant data. The potential interactions here are not fully clear, so in the interest of erring on the side of safety, make rows in these views visible only to the owner of the associated table. The PostgreSQL Project thanks Lukas Fittl for reporting this problem. (CVE-2024-4317) By itself, this fix will only fix the behavior in newly initdb'd database clusters. If you wish to apply this change in an existing cluster, you will need to do the following: 1.Find the SQL script fix-CVE-2024-4317.sql in the share directory of the PostgreSQL installation (typically located someplace like /usr/share/postgresql/ ). Be sure to use the script appropriate to your PostgreSQL major version. If you do not see this file, either your version is not vulnerable (only v14–v16 are affected) or your minor version is"
What does this passage describe?,"to avoid the overhead of materializing block data. This may make func- tion execution significantly faster. When show_data is set to false, block_data and block_fpi_data values are omitted (that is, the block_data and block_fpi_data OUT arguments are NULL for all rows returned). Obviously, this optimization is only feasible with queries where block data isn't truly required. The function raises an error if start_lsn is not available. pg_get_wal_stats(start_lsn pg_lsn, end_lsn pg_lsn, per_record boolean DEFAULT false) returns setof record Gets statistics of all the valid WAL records between start_lsn and end_lsn . By default, it returns one row per resource_manager type. When per_record is set to true, it returns one row per record_type . For example: postgres=# SELECT * FROM pg_get_wal_stats('0/1E847D00', '0/1E84F500') WHERE count > 0 AND ""resource_manager/record_type"" = 'Transaction' LIMIT 1; -[ RECORD 1 ]----------------+------------------- resource_manager/record_type | Transaction count | 2 count_percentage | 8 record_size | 875 record_size_percentage | 41.23468426013195 fpi_size | 0 fpi_size_percentage | 0 combined_size | 875 combined_size_percentage | 2.8634072910530795 The function raises an error if start_lsn is not available. 2863"
What does this passage describe?,"(whether before or after the write), it cannot see the work of the other transaction. The reader then appears to have executed first regardless of which started first or which committed first. If that is as far as it goes, there is no problem, but if the reader also writes data which is read by a concurrent transaction there is now a transaction which appears to have run before either of the previously mentioned transactions. If the transaction which appears to have executed last actually commits first, it is very easy for a cycle to appear in a graph of the order of execution of the transactions. When such a cycle appears, integrity checks will not work correctly without some help. As mentioned in Section 13.2.3 , Serializable transactions are just Repeatable Read transactions which add nonblocking monitoring for dangerous patterns of read/write conflicts. When a pattern is detected which could cause a cycle in the apparent order of execution, one of the transactions involved is rolled back to break the cycle. 13.4.1. Enforcing Consistency with Serializable Trans- actions If the Serializable transaction isolation level is used for all writes and for all reads which need a consistent view of the data, no other effort is required to ensure consistency. Software from other environments which is written to use serializable transactions to ensure consistency should “just work” in this regard in PostgreSQL. When using this technique, it will avoid creating an unnecessary burden for application program- mers if the application"
What does this passage describe?,"(tuples) in the index's parent table. A TID consists of a block number and an item number within that block (see Section 73.6 ). This is sufficient information to fetch a particular row version from the table. Indexes are not directly aware that under MVCC, there might be multiple extant ver- sions of the same logical row; to an index, each tuple is an independent object that needs its own index entry. Thus, an update of a row always creates all-new index entries for the row, even if the key values did not change. ( HOT tuples are an exception to this statement; but indexes do not deal with those, either.) Index entries for dead tuples are reclaimed (by vacuuming) when the dead tuples themselves are reclaimed. 64.1. Basic API Structure for Indexes Each index access method is described by a row in the pg_am system catalog. The pg_am entry specifies a name and a handler function for the index access method. These entries can be created and deleted using the CREATE ACCESS METHOD and DROP ACCESS METHOD SQL commands. An index access method handler function must be declared to accept a single argument of type in- ternal and to return the pseudo-type index_am_handler . The argument is a dummy value that simply serves to prevent handler functions from being called directly from SQL commands. The result of the function must be a palloc'd struct of type IndexAmRoutine , which contains everything that the core code needs to know to"
What does this passage describe?,"PL/pgSQL — SQL Procedural Language RAISE 'argument ''%'' is out of range', occur_index USING ERRCODE = '22003'; END IF; IF beg_index > 0 THEN beg := beg_index - 1; FOR i IN 1..occur_index LOOP temp_str := substring(string FROM beg + 1); pos := position(string_to_search_for IN temp_str); IF pos = 0 THEN RETURN 0; END IF; beg := beg + pos; END LOOP; RETURN beg; ELSIF beg_index < 0 THEN ss_length := char_length(string_to_search_for); length := char_length(string); beg := length + 1 + beg_index; WHILE beg > 0 LOOP temp_str := substring(string FROM beg FOR ss_length); IF string_to_search_for = temp_str THEN occur_number := occur_number + 1; IF occur_number = occur_index THEN RETURN beg; END IF; END IF; beg := beg - 1; END LOOP; RETURN 0; ELSE RETURN 0; END IF; END; $$ LANGUAGE plpgsql STRICT IMMUTABLE; 1375"
What does this passage describe?,"ECPG — Embedded SQL in C DECLARE DECLARE — define a cursor Synopsis DECLARE cursor_name [ BINARY ] [ ASENSITIVE | INSENSITIVE ] [ [ NO ] SCROLL ] CURSOR [ { WITH | WITHOUT } HOLD ] FOR prepared_name DECLARE cursor_name [ BINARY ] [ ASENSITIVE | INSENSITIVE ] [ [ NO ] SCROLL ] CURSOR [ { WITH | WITHOUT } HOLD ] FOR query Description DECLARE declares a cursor for iterating over the result set of a prepared statement. This command has slightly different semantics from the direct SQL command DECLARE : Whereas the latter executes a query and prepares the result set for retrieval, this embedded SQL command merely declares a name as a “loop variable” for iterating over the result set of a query; the actual execution happens when the cursor is opened with the OPEN command. Parameters cursor_name A cursor name, case sensitive. This can be an SQL identifier or a host variable. prepared_name The name of a prepared query, either as an SQL identifier or a host variable. query A SELECT or VALUES command which will provide the rows to be returned by the cursor. For the meaning of the cursor options, see DECLARE . Examples Examples declaring a cursor for a query: EXEC SQL DECLARE C CURSOR FOR SELECT * FROM My_Table; EXEC SQL DECLARE C CURSOR FOR SELECT Item1 FROM T; EXEC SQL DECLARE cur1 CURSOR FOR SELECT version(); An example declaring a cursor for a prepared statement: EXEC SQL PREPARE"
What does this passage describe?,"Monitoring Database Activity •You should take care that the data types specified for a probe's parameters match the data types of the variables used in the macro. Otherwise, you will get compilation errors. •On most platforms, if PostgreSQL is built with --enable-dtrace , the arguments to a trace macro will be evaluated whenever control passes through the macro, even if no tracing is being done. This is usually not worth worrying about if you are just reporting the values of a few local variables. But beware of putting expensive function calls into the arguments. If you need to do that, consider protecting the macro with a check to see if the trace is actually enabled: if (TRACE_POSTGRESQL_TRANSACTION_START_ENABLED()) TRACE_POSTGRESQL_TRANSACTION_START(some_function(...)); Each trace macro has a corresponding ENABLED macro. 852"
What does this passage describe?,"Concurrency Control updater deleted it, otherwise it will attempt to apply its operation to the updated version of the row. The search condition of the command (the WHERE clause) is re-evaluated to see if the updated version of the row still matches the search condition. If so, the second updater proceeds with its operation using the updated version of the row. In the case of SELECT FOR UPDATE and SELECT FOR SHARE, this means it is the updated version of the row that is locked and returned to the client. INSERT with an ON CONFLICT DO UPDATE clause behaves similarly. In Read Committed mode, each row proposed for insertion will either insert or update. Unless there are unrelated errors, one of those two outcomes is guaranteed. If a conflict originates in another transaction whose effects are not yet visible to the INSERT, the UPDATE clause will affect that row, even though possibly no version of that row is conventionally visible to the command. INSERT with an ON CONFLICT DO NOTHING clause may have insertion not proceed for a row due to the outcome of another transaction whose effects are not visible to the INSERT snapshot. Again, this is only the case in Read Committed mode. MERGE allows the user to specify various combinations of INSERT, UPDATE and DELETE subcom- mands. A MERGE command with both INSERT and UPDATE subcommands looks similar to INSERT with an ON CONFLICT DO UPDATE clause but does not guarantee that either INSERT or UPDATE will"
What does this passage describe?,"log file and later import the file again when it is complete, the primary key violation will cause the import to fail. Wait until the log is complete and closed before importing. This procedure will also protect against accidentally importing a partial line that hasn't been completely written, which would also cause COPY to fail. 20.8.5. Using JSON-Format Log Output Including jsonlog in the log_destination list provides a convenient way to import log files into many different programs. This option emits log lines in JSON format. String fields with null values are excluded from output. Additional fields may be added in the future. User applications that process jsonlog output should ignore unknown fields. Each log line is serialized as a JSON object with the set of keys and their associated values shown in Table 20.3 . Table 20.3. Keys and Values of JSON Log Entries Key name Type Description timestamp string Time stamp with milliseconds user string User name dbname string Database name pid number Process ID remote_host string Client host 658"
What does this passage describe?,"Release Notes In READ COMMITTED mode, an update that finds that its target row was just updated by a concur- rent transaction will recheck the query's WHERE conditions on the updated row. MERGE failed to ensure that the proper rows of other joined tables were used during this recheck, possibly resulting in incorrect decisions about whether the newly-updated row should be updated again by MERGE. •Correctly identify the target table in an inherited UPDATE/DELETE/MERGE even when the parent table is excluded by constraints (Amit Langote, Tom Lane) If the initially-named table is excluded by constraints, but not all its inheritance descendants are, the first non-excluded descendant was identified as the primary target table. This would lead to firing statement-level triggers associated with that table, rather than the initially-named table as should happen. In v16, the same oversight could also lead to “invalid perminfoindex 0 in RTE with relid NNNN” errors. •Fix edge case in btree mark/restore processing of ScalarArrayOpExpr clauses (Peter Geoghegan) When restoring an indexscan to a previously marked position, the code could miss required setup steps if the scan had advanced exactly to the end of the matches for a ScalarArrayOpExpr (that is, an indexcol = ANY(ARRAY[]) ) clause. This could result in missing some rows that should have been fetched. •Fix intra-query memory leak in Memoize execution (Orlov Aleksej, David Rowley) •Fix intra-query memory leak when a set-returning function repeatedly returns zero rows (Tom Lane) •Don't crash if cursor_to_xmlschema() is applied to a non-data-returning Portal (Boyu Yang)"
What does this passage describe?,"while (1) { EXEC SQL FETCH ... ; if (sqlca.sqlcode == ECPG_NOT_FOUND) break; } But WHENEVER NOT FOUND DO BREAK effectively does this internally, so there is usually no advantage in writing this out explicitly. -12 (ECPG_OUT_OF_MEMORY ) Indicates that your virtual memory is exhausted. The numeric value is defined as -ENOMEM . (SQLSTATE YE001) -200 (ECPG_UNSUPPORTED ) Indicates the preprocessor has generated something that the library does not know about. Perhaps you are running incompatible versions of the preprocessor and the library. (SQLSTATE YE002) -201 (ECPG_TOO_MANY_ARGUMENTS ) This means that the command specified more host variables than the command expected. (SQLS- TATE 07001 or 07002) -202 (ECPG_TOO_FEW_ARGUMENTS ) This means that the command specified fewer host variables than the command expected. (SQLS- TATE 07001 or 07002) -203 (ECPG_TOO_MANY_MATCHES ) This means a query has returned multiple rows but the statement was only prepared to store one result row (for example, because the specified variables are not arrays). (SQLSTATE 21000) 1062"
What does this passage describe?,"if the xmloption configuration parameter is set to DOCUMENT , or the latter if it is set to CONTENT . This means that xml_is_well_formed is useful for seeing whether a simple cast to type xml will succeed, whereas the other two functions are useful for seeing whether the correspond- ing variants of XMLPARSE will succeed. Examples: SET xmloption TO DOCUMENT; SELECT xml_is_well_formed('<>'); xml_is_well_formed -------------------- f (1 row) SELECT xml_is_well_formed('<abc/>'); xml_is_well_formed -------------------- t (1 row) 313"
What does this passage describe?,"Functions and Operators Function Description Example(s) age(timestamp '2001-04-10', timestamp '1957-06-13') → 43 years 9 mons 27 days age ( timestamp ) → interval Subtract argument from current_date (at midnight) age(timestamp '1957-06-13') → 62 years 6 mons 10 days clock_timestamp ( ) → timestamp with time zone Current date and time (changes during statement execution); see Section 9.9.5 clock_timestamp() → 2019-12-23 14:39:53.662522-05 current_date → date Current date; see Section 9.9.5 current_date → 2019-12-23 current_time → time with time zone Current time of day; see Section 9.9.5 current_time → 14:39:53.662522-05 current_time ( integer ) → time with time zone Current time of day, with limited precision; see Section 9.9.5 current_time(2) → 14:39:53.66-05 current_timestamp → timestamp with time zone Current date and time (start of current transaction); see Section 9.9.5 current_timestamp → 2019-12-23 14:39:53.662522-05 current_timestamp ( integer ) → timestamp with time zone Current date and time (start of current transaction), with limited precision; see Sec- tion 9.9.5 current_timestamp(0) → 2019-12-23 14:39:53-05 date_add ( timestamp with time zone , interval [, text ] ) → timestamp with time zone Add an interval to a timestamp with time zone , computing times of day and daylight-savings adjustments according to the time zone named by the third argu- ment, or the current TimeZone setting if that is omitted. The form with two arguments is equivalent to the timestamp with time zone + interval operator. date_add('2021-10-31 00:00:00+02'::timestamptz, '1 day'::interval, 'Europe/Warsaw') → 2021-10-31 23:00:00+00 date_bin ( interval , timestamp , timestamp ) → timestamp Bin input into"
What does this passage describe?,"Extending SQL 38.15.2. NEGATOR The NEGATOR clause, if provided, names an operator that is the negator of the operator being defined. We say that operator A is the negator of operator B if both return Boolean results and (x A y) equals NOT (x B y) for all possible inputs x, y. Notice that B is also the negator of A. For example, < and >= are a negator pair for most data types. An operator can never validly be its own negator. Unlike commutators, a pair of unary operators could validly be marked as each other's negators; that would mean (A x) equals NOT (B x) for all x. An operator's negator must have the same left and/or right operand types as the operator to be defined, so just as with COMMUTATOR , only the operator name need be given in the NEGATOR clause. Providing a negator is very helpful to the query optimizer since it allows expressions like NOT (x = y) to be simplified into x <> y. This comes up more often than you might think, because NOT operations can be inserted as a consequence of other rearrangements. Pairs of negator operators can be defined using the same methods explained above for commutator pairs. 38.15.3. RESTRICT The RESTRICT clause, if provided, names a restriction selectivity estimation function for the operator. (Note that this is a function name, not an operator name.) RESTRICT clauses only make sense for binary operators that return boolean . The idea behind"
What does this passage describe?,"Functions and Operators Operator Description Example(s) Computes the center point. Available for box, lseg, polygon , circle. @@ box '(2,2),(0,0)' → (1,1) # geometric_type → integer Returns the number of points. Available for path, polygon . # path '((1,0),(0,1),(-1,0))' → 3 geometric_type # geometric_type → point Computes the point of intersection, or NULL if there is none. Available for lseg, line. lseg '[(0,0),(1,1)]' # lseg '[(1,0),(0,1)]' → (0.5,0.5) box # box → box Computes the intersection of two boxes, or NULL if there is none. box '(2,2),(-1,-1)' # box '(1,1),(-2,-2)' → (1,1),(-1,-1) geometric_type ## geometric_type → point Computes the closest point to the first object on the second object. Available for these pairs of types: ( point, box), (point, lseg), (point, line), (lseg, box), (lseg, lseg), (line, lseg). point '(0,0)' ## lseg '[(2,0),(0,2)]' → (1,1) geometric_type <-> geometric_type → double precision Computes the distance between the objects. Available for all seven geometric types, for all combinations of point with another geometric type, and for these additional pairs of types: (box, lseg), (lseg, line), (polygon , circle) (and the commutator cases). circle '<(0,0),1>' <-> circle '<(5,0),1>' → 3 geometric_type @> geometric_type → boolean Does first object contain second? Available for these pairs of types: ( box, point), (box, box), (path, point), (polygon , point), (polygon , polygon ), (circle, point), (circle, circle). circle '<(0,0),2>' @> point '(1,1)' → t geometric_type <@ geometric_type → boolean Is first object contained in or on second? Available for these pairs of types: ( point, box),"
What does this passage describe?,"The Information Schema if you define your triggers in a manner that conforms with the SQL standard (trigger names unique in the schema and only one event type per trigger), this will not affect you. Note Prior to PostgreSQL 9.1, this view's columns action_timing , action_ref- erence_old_table , action_reference_new_table , action_refer- ence_old_row , and action_reference_new_row were named condition_tim- ing, condition_reference_old_table , condition_reference_new_ta- ble, condition_reference_old_row , and condition_reference_new_row respectively. That was how they were named in the SQL:1999 standard. The new naming con- forms to SQL:2003 and later. 37.58. udt_privileges The view udt_privileges identifies USAGE privileges granted on user-defined types to a cur- rently enabled role or by a currently enabled role. There is one row for each combination of type, grantor, and grantee. This view shows only composite types (see under Section 37.60 for why); see Section 37.59 for domain privileges. Table 37.56. udt_privileges Columns Column Type Description grantor sql_identifier Name of the role that granted the privilege grantee sql_identifier Name of the role that the privilege was granted to udt_catalog sql_identifier Name of the database containing the type (always the current database) udt_schema sql_identifier Name of the schema containing the type udt_name sql_identifier Name of the type privilege_type character_data Always TYPE USAGE is_grantable yes_or_no YES if the privilege is grantable, NO if not 37.59. usage_privileges The view usage_privileges identifies USAGE privileges granted on various kinds of objects to a currently enabled role or by a currently enabled role. In PostgreSQL, this currently applies to collations, domains, foreign-data wrappers, foreign"
What does this passage describe?,"ECPG — Embedded SQL in C int sqlflags; void *sqlreserved; }; struct sqlda_compat { short sqld; struct sqlvar_compat *sqlvar; char desc_name[19]; short desc_occ; struct sqlda_compat *desc_next; void *reserved; }; typedef struct sqlvar_compat sqlvar_t; typedef struct sqlda_compat sqlda_t; The global properties are: sqld The number of fields in the SQLDA descriptor. sqlvar Pointer to the per-field properties. desc_name Unused, filled with zero-bytes. desc_occ Size of the allocated structure. desc_next Pointer to the next SQLDA structure if the result set contains more than one record. reserved Unused pointer, contains NULL. Kept for Informix-compatibility. The per-field properties are below, they are stored in the sqlvar array: sqltype Type of the field. Constants are in sqltypes.h sqllen Length of the field data. sqldata Pointer to the field data. The pointer is of char * type, the data pointed by it is in a binary format. Example: int intval; switch (sqldata->sqlvar[i].sqltype) 1100"
What does this passage describe?,"pg_isready pg_isready — check the connection status of a PostgreSQL server Synopsis pg_isready [connection-option ...] [option...] Description pg_isready is a utility for checking the connection status of a PostgreSQL database server. The exit status specifies the result of the connection check. Options -d dbname --dbname=dbname Specifies the name of the database to connect to. The dbname can be a connection string . If so, connection string parameters will override any conflicting command line options. -h hostname --host=hostname Specifies the host name of the machine on which the server is running. If the value begins with a slash, it is used as the directory for the Unix-domain socket. -p port --port=port Specifies the TCP port or the local Unix-domain socket file extension on which the server is listening for connections. Defaults to the value of the PGPORT environment variable or, if not set, to the port specified at compile time, usually 5432. -q --quiet Do not display status message. This is useful when scripting. -t seconds --timeout=seconds The maximum number of seconds to wait when attempting connection before returning that the server is not responding. Setting to 0 disables. The default is 3 seconds. -U username --username=username Connect to the database as the user username instead of the default. -V --version Print the pg_isready version and exit. -? --help Show help about pg_isready command line arguments, and exit. 2113"
What does this passage describe?,"Monitoring Database Activity View Name Description pg_stat_user_functions One row for each tracked function, showing sta- tistics about executions of that function. See pg_stat_user_functions for details. pg_stat_xact_user_functions Similar to pg_stat_user_functions , but counts only calls during the current trans- action (which are not yet included in pg_s- tat_user_functions ). pg_statio_all_tables One row for each table in the current database, showing statistics about I/O on that specific ta- ble. See pg_statio_all_tables for de- tails. pg_statio_sys_tables Same as pg_statio_all_tables , except that only system tables are shown. pg_statio_user_tables Same as pg_statio_all_tables , except that only user tables are shown. pg_statio_all_indexes One row for each index in the current database, showing statistics about I/O on that specific in- dex. See pg_statio_all_indexes for de- tails. pg_statio_sys_indexes Same as pg_statio_all_indexes , except that only indexes on system tables are shown. pg_statio_user_indexes Same as pg_statio_all_indexes , except that only indexes on user tables are shown. pg_statio_all_sequences One row for each sequence in the current data- base, showing statistics about I/O on that spe- cific sequence. See pg_statio_all_se- quences for details. pg_statio_sys_sequences Same as pg_statio_all_sequences , except that only system sequences are shown. (Presently, no system sequences are defined, so this view is always empty.) pg_statio_user_sequences Same as pg_statio_all_sequences , ex- cept that only user sequences are shown. The per-index statistics are particularly useful to determine which indexes are being used and how effective they are. The pg_stat_io and pg_statio_ set of views are useful for determining the effectiveness of the buffer cache. They can be used to"
What does this passage describe?,"remote sampling, so all data are transferred and sampled locally. random performs remote sampling using the random() function to choose returned rows, while system and bernoulli rely on the built-in TABLESAMPLE methods of those names. random works on all remote server versions, while TABLESAMPLE is supported only since 9.5. auto (the default) picks the recommended sampling method automatically; currently it means either bernoulli or random depending on the remote server version. F.38.1.4. Remote Execution Options By default, only WHERE clauses using built-in operators and functions will be considered for execu- tion on the remote server. Clauses involving non-built-in functions are checked locally after rows are fetched. If such functions are available on the remote server and can be relied on to produce the same results as they do locally, performance can be improved by sending such WHERE clauses for remote execution. This behavior can be controlled using the following option: extensions (string) This option is a comma-separated list of names of PostgreSQL extensions that are installed, in compatible versions, on both the local and remote servers. Functions and operators that are im- mutable and belong to a listed extension will be considered shippable to the remote server. This option can only be specified for foreign servers, not per-table. 2867"
What does this passage describe?,"update 2.The new value of the column 3.Which row(s) to update Recall from Chapter 5 that SQL does not, in general, provide a unique identifier for rows. Therefore it is not always possible to directly specify which row to update. Instead, you specify which conditions a row must meet in order to be updated. Only if you have a primary key in the table (independent of whether you declared it or not) can you reliably address individual rows by choosing a condition that matches the primary key. Graphical database access tools rely on this fact to allow you to update rows individually. For example, this command updates all products that have a price of 5 to have a price of 10: UPDATE products SET price = 10 WHERE price = 5; This might cause zero, one, or many rows to be updated. It is not an error to attempt an update that does not match any rows. Let's look at that command in detail. First is the key word UPDATE followed by the table name. As usual, the table name can be schema-qualified, otherwise it is looked up in the path. Next is the key word SET followed by the column name, an equal sign, and the new column value. The new column value can be any scalar expression, not just a constant. For example, if you want to raise the price of all products by 10% you could use: 111"
What does this passage describe?,"made for it, but the index contents are not initialized by this command. declare toast toasttableoid toastindexoid on tablename Create a TOAST table for the table named tablename . The TOAST table is assigned OID toasttableoid and its index is assigned OID toastindexoid . As with declare in- dex, filling of the index is postponed. build indices Fill in the indices that have previously been declared. 75.5. Structure of the Bootstrap BKI File The open command cannot be used until the tables it uses exist and have entries for the table that is to be opened. (These minimum tables are pg_class , pg_attribute , pg_proc , and pg_type .) To allow those tables themselves to be filled, create with the bootstrap option implicitly opens the created table for data insertion. Also, the declare index and declare toast commands cannot be used until the system catalogs they need have been created and filled in. Thus, the structure of the postgres.bki file has to be: 1.create bootstrap one of the critical tables 2.insert data describing at least the critical tables 3.close 4.Repeat for the other critical tables. 5.create (without bootstrap ) a noncritical table 6.open 2541"
What does this passage describe?,the schema to which the table will be moved. Notes The key word COLUMN is noise and can be omitted. 1549
What does this passage describe?,"handle binary cursors and expect data to come back in the text format. Note When the client application uses the “extended query” protocol to issue a FETCH command, the Bind protocol message specifies whether data is to be retrieved in text or binary format. This choice overrides the way that the cursor is defined. The concept of a binary cursor as such is thus obsolete when using extended query protocol — any cursor can be treated as either text or binary. Unless WITH HOLD is specified, the cursor created by this command can only be used within the current transaction. Thus, DECLARE without WITH HOLD is useless outside a transaction block: the cursor would survive only to the completion of the statement. Therefore PostgreSQL reports an error if such a command is used outside a transaction block. Use BEGIN and COMMIT (or ROLLBACK ) to define a transaction block. If WITH HOLD is specified and the transaction that created the cursor successfully commits, the cursor can continue to be accessed by subsequent transactions in the same session. (But if the creating transaction is aborted, the cursor is removed.) A cursor created with WITH HOLD is closed when an explicit CLOSE command is issued on it, or the session ends. In the current implementation, the rows represented by a held cursor are copied into a temporary file or memory area so that they remain available for subsequent transactions. WITH HOLD may not be specified when the query includes FOR UPDATE or"
What does this passage describe?,"Functions and Operators Function Description Returns the set of in-progress transaction IDs contained in a snapshot. pg_snapshot_xmax ( pg_snapshot ) → xid8 Returns the xmax of a snapshot. pg_snapshot_xmin ( pg_snapshot ) → xid8 Returns the xmin of a snapshot. pg_visible_in_snapshot ( xid8, pg_snapshot ) → boolean Is the given transaction ID visible according to this snapshot (that is, was it completed before the snapshot was taken)? Note that this function will not give the correct answer for a subtransaction ID (subxid); see Section 74.3 for details. pg_get_multixact_members ( multixid xid ) → setof record ( xid xid, mode text ) Returns the transaction ID and lock mode for each member of the specified multixact ID. The lock modes forupd, fornokeyupd , sh, and keysh correspond to the row-level locks FOR UPDATE , FOR NO KEY UPDATE , FOR SHARE , and FOR KEY SHARE , respectively, as described in Section 13.3.2 . Two additional modes are specific to multi- xacts: nokeyupd , used by updates that do not modify key columns, and upd, used by updates or deletes that modify key columns. The internal transaction ID type xid is 32 bits wide and wraps around every 4 billion transactions. However, the functions shown in Table 9.80 , except age, mxid_age , and pg_get_multixac- t_members , use a 64-bit type xid8 that does not wrap around during the life of an installation and can be converted to xid by casting if required; see Section 74.1 for details. The data type"
What does this passage describe?,"atop the functions, and if needed, operator classes can be created to support indexing of the data type. These additional layers are discussed in following sections. If the internal representation of the data type is variable-length, the internal representation must follow the standard layout for variable-length data: the first four bytes must be a char[4] field which is never accessed directly (customarily named vl_len_ ). You must use the SET_VARSIZE() macro to store the total size of the datum (including the length field itself) in this field and VARSIZE() to retrieve it. (These macros exist because the length field may be encoded depending on platform.) For further details see the description of the CREATE TYPE command. 1227"
What does this passage describe?,"Functions and Operators Function Description Example(s) Inverse cosine, result in radians acos(1) → 0 acosd ( double precision ) → double precision Inverse cosine, result in degrees acosd(0.5) → 60 asin ( double precision ) → double precision Inverse sine, result in radians asin(1) → 1.5707963267948966 asind ( double precision ) → double precision Inverse sine, result in degrees asind(0.5) → 30 atan ( double precision ) → double precision Inverse tangent, result in radians atan(1) → 0.7853981633974483 atand ( double precision ) → double precision Inverse tangent, result in degrees atand(1) → 45 atan2 ( y double precision , x double precision ) → double precision Inverse tangent of y/x, result in radians atan2(1, 0) → 1.5707963267948966 atan2d ( y double precision , x double precision ) → double precision Inverse tangent of y/x, result in degrees atan2d(1, 0) → 90 cos ( double precision ) → double precision Cosine, argument in radians cos(0) → 1 cosd ( double precision ) → double precision Cosine, argument in degrees cosd(60) → 0.5 cot ( double precision ) → double precision Cotangent, argument in radians cot(0.5) → 1.830487721712452 cotd ( double precision ) → double precision Cotangent, argument in degrees cotd(45) → 1 sin ( double precision ) → double precision Sine, argument in radians sin(1) → 0.8414709848078965 sind ( double precision ) → double precision Sine, argument in degrees 229"
What does this passage describe?,"INSERT OVERRIDING USER VALUE . (For an identity column defined as GENERATED BY DEFAULT , OVERRIDING SYSTEM VALUE is the normal behavior and specifying it does nothing, but PostgreSQL allows it as an extension.) OVERRIDING USER VALUE If this clause is specified, then any values supplied for identity columns are ignored and the default sequence-generated values are applied. This clause is useful for example when copying values between tables. Writing INSERT INTO tbl2 OVERRIDING USER VALUE SELECT * FROM tbl1 will copy from tbl1 all columns that are not identity columns in tbl2 while values for the identity columns in tbl2 will be generated by the sequences associated with tbl2. DEFAULT VALUES All columns will be filled with their default values, as if DEFAULT were explicitly specified for each column. (An OVERRIDING clause is not permitted in this form.) expression An expression or value to assign to the corresponding column. DEFAULT The corresponding column will be filled with its default value. An identity column will be filled with a new value generated by the associated sequence. For a generated column, specifying this is permitted but merely specifies the normal behavior of computing the column from its generation expression. query A query ( SELECT statement) that supplies the rows to be inserted. Refer to the SELECT statement for a description of the syntax. output_expression An expression to be computed and returned by the INSERT command after each row is inserted or updated. The expression can use any column names of the"
What does this passage describe?,"report; the execution counts will ac- cumulate. If you want to reset the execution counts between test runs, run: make coverage-clean You can run the make coverage-html or make coverage command in a subdirectory if you want a coverage report for only a portion of the code tree. Use make distclean to clean up when done. 33.5.2. Coverage with Meson A typical workflow looks like this: meson setup -Db_coverage=true ... OTHER OPTIONS ... builddir/ meson compile -C builddir/ meson test -C builddir/ cd builddir/ ninja coverage-html Then point your HTML browser to ./meson-logs/coveragereport/index.html . You can run several different tests before making the coverage report; the execution counts will ac- cumulate. 898"
What does this passage describe?,"information, see Section 30.6 . WAL record A low-level description of an individual data change. It contains sufficient information for the data change to be re-executed ( re- played) in case a system failure causes the change to be lost. WAL records use a non-printable binary format. For more information, see Section 30.6 . 2952"
What does this passage describe?,"methods can be selected with the -m option. “Smart” mode disallows new connections, then waits for all existing clients to disconnect. If the server is in hot standby, recovery and streaming repli- 2201"
What does this passage describe?,"psql testdb=> \setenv PAGER less testdb=> \setenv LESS -imx4F \sf[+] function_description This command fetches and shows the definition of the named function or procedure, in the form of a CREATE OR REPLACE FUNCTION or CREATE OR REPLACE PROCEDURE command. The definition is printed to the current query output channel, as set by \o. The target function can be specified by name alone, or by name and arguments, for example foo(integer, text) . The argument types must be given if there is more than one function of the same name. If + is appended to the command name, then the output lines are numbered, with the first line of the function body being line 1. Unlike most other meta-commands, the entire remainder of the line is always taken to be the argument(s) of \sf, and neither variable interpolation nor backquote expansion are performed in the arguments. \sv[+] view_name This command fetches and shows the definition of the named view, in the form of a CREATE OR REPLACE VIEW command. The definition is printed to the current query output channel, as set by \o. If + is appended to the command name, then the output lines are numbered from 1. Unlike most other meta-commands, the entire remainder of the line is always taken to be the argument(s) of \sv, and neither variable interpolation nor backquote expansion are performed in the arguments. \t Toggles the display of output column name headings and row count footer. This command is equivalent to \pset tuples_only and"
What does this passage describe?,Server Programming 50. Replication Progress Tracking ...............................................................................1508 51. Archive Modules ................................................................................................1509 51.1. Initialization Functions ..............................................................................1509 51.2. Archive Module Callbacks .........................................................................1509 51.2.1. Startup Callback ............................................................................1509 51.2.2. Check Callback .............................................................................1510 51.2.3. Archive Callback ...........................................................................1510 51.2.4. Shutdown Callback ........................................................................1510 1171
What does this passage describe?,"appear in that stan- dard, but have slightly different though related meanings. Some of the concurrency behavior of this command is left implementation-defined by the standard, so the above notes should be considered and compared with other implementations if necessary. See Also DELETE 2011"
What does this passage describe?,"Server Setup and Operation -keyout server.key -subj ""/CN= dbhost.yourdomain.com "" Then do: chmod og-rwx server.key because the server will reject the file if its permissions are more liberal than this. For more details on how to create your server private key and certificate, refer to the OpenSSL documentation. While a self-signed certificate can be used for testing, a certificate signed by a certificate authority (CA) (usually an enterprise-wide root CA) should be used in production. To create a server certificate whose identity can be validated by clients, first create a certificate signing request (CSR) and a public/private key file: openssl req -new -nodes -text -out root.csr \ -keyout root.key -subj ""/CN= root.yourdomain.com "" chmod og-rwx root.key Then, sign the request with the key to create a root certificate authority (using the default OpenSSL configuration file location on Linux): openssl x509 -req -in root.csr -text -days 3650 \ -extfile /etc/ssl/openssl.cnf -extensions v3_ca \ -signkey root.key -out root.crt Finally, create a server certificate signed by the new root certificate authority: openssl req -new -nodes -text -out server.csr \ -keyout server.key -subj ""/CN= dbhost.yourdomain.com "" chmod og-rwx server.key openssl x509 -req -in server.csr -text -days 365 \ -CA root.crt -CAkey root.key -CAcreateserial \ -out server.crt server.crt and server.key should be stored on the server, and root.crt should be stored on the client so the client can verify that the server's leaf certificate was signed by its trusted root certificate. root.key should be stored offline for use in creating future certificates. It is also"
What does this passage describe?,"Generic WAL Records •Since you are modifying copies of buffer pages, GenericXLogStart() does not start a crit- ical section. Thus, you can safely do memory allocation, error throwing, etc. between Gener- icXLogStart() and GenericXLogFinish() . The only actual critical section is present inside GenericXLogFinish() . There is no need to worry about calling GenericXLo- gAbort() during an error exit, either. •GenericXLogFinish() takes care of marking buffers dirty and setting their LSNs. You do not need to do this explicitly. •For unlogged relations, everything works the same except that no actual WAL record is emitted. Thus, you typically do not need to do any explicit checks for unlogged relations. •The generic WAL redo function will acquire exclusive locks to buffers in the same order as they were registered. After redoing all changes, the locks will be released in the same order. •If GENERIC_XLOG_FULL_IMAGE is not specified for a registered buffer, the generic WAL record contains a delta between the old and the new page images. This delta is based on byte-by-byte comparison. This is not very compact for the case of moving data within a page, and might be improved in the future. 2460"
What does this passage describe?,"Data Types SELECT ARRAY[1,2] || ARRAY[3,4]; ?column? ----------- {1,2,3,4} (1 row) SELECT ARRAY[5,6] || ARRAY[[1,2],[3,4]]; ?column? --------------------- {{5,6},{1,2},{3,4}} (1 row) The concatenation operator allows a single element to be pushed onto the beginning or end of a one- dimensional array. It also accepts two N-dimensional arrays, or an N-dimensional and an N+1-dimen- sional array. When a single element is pushed onto either the beginning or end of a one-dimensional array, the result is an array with the same lower bound subscript as the array operand. For example: SELECT array_dims(1 || '[0:1]={2,3}'::int[]); array_dims ------------ [0:2] (1 row) SELECT array_dims(ARRAY[1,2] || 3); array_dims ------------ [1:3] (1 row) When two arrays with an equal number of dimensions are concatenated, the result retains the lower bound subscript of the left-hand operand's outer dimension. The result is an array comprising every element of the left-hand operand followed by every element of the right-hand operand. For example: SELECT array_dims(ARRAY[1,2] || ARRAY[3,4,5]); array_dims ------------ [1:5] (1 row) SELECT array_dims(ARRAY[[1,2],[3,4]] || ARRAY[[5,6],[7,8],[9,0]]); array_dims ------------ [1:5][1:2] (1 row) When an N-dimensional array is pushed onto the beginning or end of an N+1-dimensional array, the result is analogous to the element-array case above. Each N-dimensional sub-array is essentially an element of the N+1-dimensional array's outer dimension. For example: SELECT array_dims(ARRAY[1,2] || ARRAY[[3,4],[5,6]]); array_dims ------------ [1:3][1:2] 196"
What does this passage describe?,"to refer to the table by the original name elsewhere in the query. Thus, this is not valid: SELECT * FROM my_table AS m WHERE my_table.a > 5; -- wrong Table aliases are mainly for notational convenience, but it is necessary to use them when joining a table to itself, e.g.: SELECT * FROM people AS mother JOIN people AS child ON mother.id = child.mother_id; Parentheses are used to resolve ambiguities. In the following example, the first statement assigns the alias b to the second instance of my_table , but the second statement assigns the alias to the result of the join: SELECT * FROM my_table AS a CROSS JOIN my_table AS b ... SELECT * FROM (my_table AS a CROSS JOIN my_table) AS b ... Another form of table aliasing gives temporary names to the columns of the table, as well as the table itself: FROM table_reference [AS] alias ( column1 [, column2 [, ...]] ) 119"
What does this passage describe?,"Whether the sequence cycles cache_size int8 Cache size of the sequence last_value int8 The last sequence value written to disk. If caching is used, this value can be greater than the last value handed out from the sequence. The last_value column will read as null if any of the following are true: •The sequence has not been read from yet. 2321"
What does this passage describe?,same map-name can be used repeatedly to specify multiple user-mappings within a single map. 695
What does this passage describe?,"Additional Supplied Modules and Extensions F.6. basic_archive — an example WAL archive module basic_archive is an example of an archive module. This module copies completed WAL segment files to the specified directory. This may not be especially useful, but it can serve as a starting point for developing your own archive module. For more information about archive modules, see Chapter 51 . In order to function, this module must be loaded via archive_library , and archive_mode must be en- abled. F.6.1. Configuration Parameters basic_archive.archive_directory (string) The directory where the server should copy WAL segment files. This directory must already exist. The default is an empty string, which effectively halts WAL archiving, but if archive_mode is enabled, the server will accumulate WAL segment files in the expectation that a value will soon be provided. These parameters must be set in postgresql.conf . Typical usage might be: # postgresql.conf archive_mode = 'on' archive_library = 'basic_archive' basic_archive.archive_directory = '/path/to/archive/directory' F.6.2. Notes Server crashes may leave temporary files with the prefix archtemp in the archive directory. It is recommended to delete such files before restarting the server after a crash. It is safe to remove such files while the server is running as long as they are unrelated to any archiving still in progress, but users should use extra caution when doing so. F.6.3. Author Nathan Bossart 2716"
What does this passage describe?,"Chapter 32. Just-in-Time Compilation (JIT) This chapter explains what just-in-time compilation is, and how it can be configured in PostgreSQL. 32.1. What Is JIT compilation? Just-in-Time (JIT) compilation is the process of turning some form of interpreted program evaluation into a native program, and doing so at run time. For example, instead of using general-purpose code that can evaluate arbitrary SQL expressions to evaluate a particular SQL predicate like WHERE a.col = 3, it is possible to generate a function that is specific to that expression and can be natively executed by the CPU, yielding a speedup. PostgreSQL has builtin support to perform JIT compilation using LLVM1 when PostgreSQL is built with --with-llvm . See src/backend/jit/README for further details. 32.1.1. JIT Accelerated Operations Currently PostgreSQL's JIT implementation has support for accelerating expression evaluation and tuple deforming. Several other operations could be accelerated in the future. Expression evaluation is used to evaluate WHERE clauses, target lists, aggregates and projections. It can be accelerated by generating code specific to each case. Tuple deforming is the process of transforming an on-disk tuple (see Section 73.6.1 ) into its in-memory representation. It can be accelerated by creating a function specific to the table layout and the number of columns to be extracted. 32.1.2. Inlining PostgreSQL is very extensible and allows new data types, functions, operators and other database objects to be defined; see Chapter 38 . In fact the built-in objects are implemented using nearly the same mechanisms. This extensibility implies some overhead, for"
What does this passage describe?,"from the start, end, or both ends ( BOTH is the default) of string. trim(both 'xyz' from 'yxTomxx') → Tom trim ( [ LEADING | TRAILING | BOTH ] [ FROM ] string text [, characters text ] ) → text This is a non-standard syntax for trim(). trim(both from 'yxTomxx', 'xyz') → Tom upper ( text ) → text Converts the string to all upper case, according to the rules of the database's locale. upper('tom') → TOM Additional string manipulation functions and operators are available and are listed in Table 9.10 . (Some of these are used internally to implement the SQL-standard string functions listed in Table 9.9.) There are also pattern-matching operators, which are described in Section 9.7 , and operators for full- text search, which are described in Chapter 12 . Table 9.10. Other String Functions and Operators Function/Operator Description Example(s) text ^@ text → boolean Returns true if the first string starts with the second string (equivalent to the start- s_with() function). 'alphabet' ^@ 'alph' → t 233"
What does this passage describe?,"superusers, since those bypass all permission checks). To create such a role, use CREATE ROLE name CREATE- ROLE. A role with CREATEROLE privilege can alter and drop roles which have been granted to the CREATEROLE user with the ADMIN option. Such a grant occurs automatically when a CRE- ATEROLE user that is not a superuser creates a new role, so that by default, a CREATEROLE user can alter and drop the roles which they have created. Altering a role includes most changes that can be made using ALTER ROLE , including, for example, changing passwords. It also includes modifications to a role that can be made using the COMMENT and SECURITY LABEL commands. However, CREATEROLE does not convey the ability to create SUPERUSER roles, nor does it convey any power over SUPERUSER roles that already exist. Furthermore, CREATEROLE does not convey the power to create REPLICATION users, nor the ability to grant or revoke the RE- PLICATION privilege, nor the ability to modify the role properties of such users. However, it 710"
What does this passage describe?,"is used. If all argument values have the standalone declaration value “yes”, then that value is used in the result. If all argument values have a standalone declaration value and at least one is “no”, then that is used in the result. Else the result will have no standalone declaration. If the result is determined to require a standalone declaration but no version declaration, a version declaration with version 1.0 will be used because XML requires an XML declaration to contain a version declaration. Encoding declarations are ignored and removed in all cases. Example: 308"
What does this passage describe?,"the same methods explained above for commutator pairs. 38.15.3. RESTRICT The RESTRICT clause, if provided, names a restriction selectivity estimation function for the operator. (Note that this is a function name, not an operator name.) RESTRICT clauses only make sense for binary operators that return boolean . The idea behind a restriction selectivity estimator is to guess what fraction of the rows in a table will satisfy a WHERE-clause condition of the form: column OP constant for the current operator and a particular constant value. This assists the optimizer by giving it some idea of how many rows will be eliminated by WHERE clauses that have this form. (What happens if the constant is on the left, you might be wondering? Well, that's one of the things that COMMUTATOR is for...) Writing new restriction selectivity estimation functions is far beyond the scope of this chapter, but fortunately you can usually just use one of the system's standard estimators for many of your own operators. These are the standard restriction estimators: eqsel for = neqsel for <> scalarltsel for < scalarlesel for <= scalargtsel for > scalargesel for >= You can frequently get away with using either eqsel or neqsel for operators that have very high or very low selectivity, even if they aren't really equality or inequality. For example, the approxi- mate-equality geometric operators use eqsel on the assumption that they'll usually only match a small fraction of the entries in a table. You can use scalarltsel , scalarlesel ,"
What does this passage describe?,"Chapter 29. Monitoring Disk Usage This chapter discusses how to monitor the disk usage of a PostgreSQL database system. 29.1. Determining Disk Usage Each table has a primary heap disk file where most of the data is stored. If the table has any columns with potentially-wide values, there also might be a TOAST file associated with the table, which is used to store values too wide to fit comfortably in the main table (see Section 73.2 ). There will be one valid index on the TOAST table, if present. There also might be indexes associated with the base table. Each table and index is stored in a separate disk file — possibly more than one file, if the file would exceed one gigabyte. Naming conventions for these files are described in Section 73.1 . You can monitor disk space in three ways: using the SQL functions listed in Table 9.96 , using the oid2name module, or using manual inspection of the system catalogs. The SQL functions are the easiest to use and are generally recommended. The remainder of this section shows how to do it by inspection of the system catalogs. Using psql on a recently vacuumed or analyzed database, you can issue queries to see the disk usage of any table: SELECT pg_relation_filepath(oid), relpages FROM pg_class WHERE relname = 'customer'; pg_relation_filepath | relpages ----------------------+---------- base/16384/16806 | 60 (1 row) Each page is typically 8 kilobytes. (Remember, relpages is only updated by VACUUM, ANALYZE , and a few DDL commands"
What does this passage describe?,"argmode The mode of an argument: IN or VARIADIC . If omitted, the default is IN. argname The name of an argument. Note that ALTER AGGREGATE does not actually pay any attention to argument names, since only the argument data types are needed to determine the aggregate function's identity. argtype An input data type on which the aggregate function operates. To reference a zero-argument aggre- gate function, write * in place of the list of argument specifications. To reference an ordered-set aggregate function, write ORDER BY between the direct and aggregated argument specifications. new_name The new name of the aggregate function. 1521"
What does this passage describe?,"OID- referencing system data types: regcollation regconfig regdictionary regnamespace regoper regoperator regproc regprocedure (regclass , regrole , and regtype can be upgraded.) If you want to use link mode and you do not want your old cluster to be modified when the new cluster is started, consider using the clone mode. If that is not available, make a copy of the old cluster and upgrade that in link mode. To make a valid copy of the old cluster, use rsync to create a dirty copy of the old cluster while the server is running, then shut down the old server and run rsync --checksum again to update the copy with any changes to make it consistent. ( --checksum is 2227"
What does this passage describe?,"87 NaN (see not a number ) natural join, 117 negation, 218 NetBSD IPC configuration, 584 shared library, 1204 start script, 580 netmask, 300 network, 300 data types, 172 nextval, 342 NFS, 578 nlevel, 2800 non-durable, 524 nonblocking connection, 906, 945 nonrepeatable read, 486 normalize, 232 normalized, 231 normal_rand, 2890 NOT (operator), 218 not a number floating point, 150 numeric (data type), 148 NOT IN, 364, 367 not-null constraint, 65 notation functions, 56 notice processing in libpq, 967 notice processor, 967 notice receiver, 967 NOTIFY, 1939 in libpq, 956 NOTNULL, 221 now, 279 npoints, 295 nth_value, 364 ntile, 363 2972"
What does this passage describe?,"Extending SQL Sometimes it is handy to construct a composite argument value on-the-fly. This can be done with the ROW construct. For example, we could adjust the data being passed to the function: SELECT name, double_salary(ROW(name, salary*1.1, age, cubicle)) AS dream FROM emp; It is also possible to build a function that returns a composite type. This is an example of a function that returns a single emp row: CREATE FUNCTION new_emp() RETURNS emp AS $$ SELECT text 'None' AS name, 1000.0 AS salary, 25 AS age, point '(2,2)' AS cubicle; $$ LANGUAGE SQL; In this example we have specified each of the attributes with a constant value, but any computation could have been substituted for these constants. Note two important things about defining the function: •The select list order in the query must be exactly the same as that in which the columns appear in the composite type. (Naming the columns, as we did above, is irrelevant to the system.) •We must ensure each expression's type can be cast to that of the corresponding column of the composite type. Otherwise we'll get errors like this: ERROR: return type mismatch in function declared to return emp DETAIL: Final statement returns text instead of point at column 4. As with the base-type case, the system will not insert explicit casts automatically, only implicit or assignment casts. A different way to define the same function is: CREATE FUNCTION new_emp() RETURNS emp AS $$ SELECT ROW('None', 1000.0, 25, '(2,2)')::emp; $$ LANGUAGE SQL; Here"
What does this passage describe?,"see join plan nodes with both “Join Filter” and plain “Filter” conditions attached. Join Filter conditions come from the outer join's ON clause, so a row that fails the Join Filter condition could still get emitted as a null-extended row. But a plain Filter condition is applied after the outer-join rules and so acts to remove rows unconditionally. In an inner join there is no semantic difference between these types of filters. If we change the query's selectivity a bit, we might get a very different join plan: EXPLAIN SELECT * FROM tenk1 t1, tenk2 t2 WHERE t1.unique1 < 100 AND t1.unique2 = t2.unique2; QUERY PLAN ------------------------------------------------------------------- ----------------------- Hash Join (cost=230.47..713.98 rows=101 width=488) Hash Cond: (t2.unique2 = t1.unique2) -> Seq Scan on tenk2 t2 (cost=0.00..445.00 rows=10000 width=244) -> Hash (cost=229.20..229.20 rows=101 width=244) -> Bitmap Heap Scan on tenk1 t1 (cost=5.07..229.20 rows=101 width=244) Recheck Cond: (unique1 < 100) -> Bitmap Index Scan on tenk1_unique1 (cost=0.00..5.04 rows=101 width=0) Index Cond: (unique1 < 100) Here, the planner has chosen to use a hash join, in which rows of one table are entered into an in- memory hash table, after which the other table is scanned and the hash table is probed for matches to each row. Again note how the indentation reflects the plan structure: the bitmap scan on tenk1 is the 507"
What does this passage describe?,"Extending SQL VARHDRSZ is the same as sizeof(int32) , but it's considered good style to use the macro VARHDRSZ to refer to the size of the overhead for a variable-length type. Also, the length field must be set using the SET_VARSIZE macro, not by simple assignment. Table 38.2 shows the C types corresponding to many of the built-in SQL data types of PostgreSQL. The “Defined In” column gives the header file that needs to be included to get the type definition. (The actual definition might be in a different file that is included by the listed file. It is recommended that users stick to the defined interface.) Note that you should always include postgres.h first in any source file of server code, because it declares a number of things that you will need anyway, and because including other headers first can cause portability issues. Table 38.2. Equivalent C Types for Built-in SQL Types SQL Type C Type Defined In boolean bool postgres.h (maybe compiler built-in) box BOX* utils/geo_decls.h bytea bytea* postgres.h ""char"" char (compiler built-in) character BpChar* postgres.h cid CommandId postgres.h date DateADT utils/date.h float4 (real) float4 postgres.h float8 (double precision )float8 postgres.h int2 (smallint )int16 postgres.h int4 (integer )int32 postgres.h int8 (bigint) int64 postgres.h interval Interval* datatype/timestamp.h lseg LSEG* utils/geo_decls.h name Name postgres.h numeric Numeric utils/numeric.h oid Oid postgres.h oidvector oidvector* postgres.h path PATH* utils/geo_decls.h point POINT* utils/geo_decls.h regproc RegProcedure postgres.h text text* postgres.h tid ItemPointer storage/itemptr.h time TimeADT utils/date.h time with time zoneTimeTzADT utils/date.h timestamp Timestamp datatype/timestamp.h timestamp"
What does this passage describe?,"used by the function (always the cur- rent database) table_schema sql_identifier Name of the schema that contains the table that is used by the function table_name sql_identifier Name of the table that is used by the function column_name sql_identifier Name of the column that is used by the function 37.41. routine_privileges The view routine_privileges identifies all privileges granted on functions to a currently en- abled role or by a currently enabled role. There is one row for each combination of function, grantor, and grantee. 1144"
What does this passage describe?,"also be better modeled with a higher value for random_page_cost. Correspondingly, if your data is likely to be completely in cache, such as when the database is smaller than the total server memory, or network latency is high, decreasing random_page_cost might be appropriate. Tip Although the system will let you set random_page_cost to less than se- q_page_cost , it is not physically sensible to do so. However, setting them equal makes sense if the database is entirely cached in RAM, since in that case there is no penalty for touching pages out of sequence. Also, in a heavily-cached database you should lower both values relative to the CPU parameters, since the cost of fetching a page already in RAM is much smaller than it would normally be. cpu_tuple_cost (floating point ) Sets the planner's estimate of the cost of processing each row during a query. The default is 0.01. cpu_index_tuple_cost (floating point ) Sets the planner's estimate of the cost of processing each index entry during an index scan. The default is 0.005. cpu_operator_cost (floating point ) Sets the planner's estimate of the cost of processing each operator or function executed during a query. The default is 0.0025. parallel_setup_cost (floating point ) Sets the planner's estimate of the cost of launching parallel worker processes. The default is 1000. parallel_tuple_cost (floating point ) Sets the planner's estimate of the cost of transferring one tuple from a parallel worker process to another process. The default is 0.1. min_parallel_table_scan_size (integer ) Sets the"
What does this passage describe?,"responds with a single byte containing G or N, indicat- ing that it is willing or unwilling to perform GSSAPI encryption, respectively. The frontend might close the connection at this point if it is dissatisfied with the response. To continue after G, using the GSSAPI C bindings as discussed in RFC 27443 or equivalent, perform a GSSAPI initialization by calling gss_init_sec_context() in a loop and sending the result to the server, starting with an empty input and then with each result from the server, until it returns no output. When sending the results of gss_init_sec_context() to the server, prepend the length of the message as a four byte integer in network byte order. To continue after N, send the usual StartupMessage and pro- ceed without encryption. (Alternatively, it is permissible to issue an SSLRequest message after an N response to try to use SSL encryption instead of GSSAPI.) The frontend should also be prepared to handle an ErrorMessage response to GSSENCRequest from the server. The frontend should not display this error message to the user/application, since the server has not been authenticated ( CVE-2024-109774). In this case the connection must be closed, but the frontend might choose to open a fresh connection and proceed without requesting GSSAPI encryption. When GSSAPI encryption can be performed, the server is expected to send only the single G byte and then wait for the frontend to initiate a GSSAPI handshake. If additional bytes are available to read at 1 https://www.postgresql.org/support/security/CVE-2024-10977/ 2 https://www.postgresql.org/support/security/CVE-2021-23222/ 3 https://datatracker.ietf.org/doc/html/rfc2744"
What does this passage describe?,last value set for a transaction commit timestamp. CommitTsBuffer Waiting for I/O on a commit timestamp SLRU buffer. CommitTsSLRU Waiting to access the commit timestamp SLRU cache. ControlFile Waiting to read or update the pg_control file or create a new WAL file. DynamicSharedMemoryControl Waiting to read or update dynamic shared mem- ory allocation information. LockFastPath Waiting to read or update a process' fast-path lock information. LockManager Waiting to read or update information about “heavyweight” locks. LogicalRepLauncherDSA Waiting to access logical replication launcher's dynamic shared memory allocator. 810
What does this passage describe?,"if amcanbackward is false, all subsequent calls will have the same direction as the first one.) Access methods that support ordered scans must support “marking” a position in a scan and later returning to the marked position. The same position might be restored multiple times. However, only one position need be remembered per scan; a new ammarkpos call overrides the previously marked position. An access method that does not support ordered scans need not provide ammarkpos and amrestrpos functions in IndexAmRoutine ; set those pointers to NULL instead. 2452"
What does this passage describe?,"the situation. This is a bit confusing. Examples are psql.po (PO file for psql) or fr.mo (MO file in French). The file format of the PO files is illustrated here: # comment msgid ""original string"" msgstr ""translated string"" msgid ""more original"" msgstr ""another translated"" ""string can be broken up like this"" ... The msgid lines are extracted from the program source. (They need not be, but this is the most common way.) The msgstr lines are initially empty and are filled in with useful strings by the translator. The strings can contain C-style escape characters and can be continued across lines as illustrated. (The next line must start at the beginning of the line.) 2400"
What does this passage describe?,"System Catalogs Column Type Description enumsortorder float4 The sort position of this enum value within its enum type enumlabel name The textual label for this enum value The OIDs for pg_enum rows follow a special rule: even-numbered OIDs are guaranteed to be ordered in the same way as the sort ordering of their enum type. That is, if two even OIDs belong to the same enum type, the smaller OID must have the smaller enumsortorder value. Odd-numbered OID values need bear no relationship to the sort order. This rule allows the enum comparison routines to avoid catalog lookups in many common cases. The routines that create and alter enum types attempt to assign even OIDs to enum values whenever possible. When an enum type is created, its members are assigned sort-order positions 1.. n. But members added later might be given negative or fractional values of enumsortorder . The only requirement on these values is that they be correctly ordered and unique within each enum type. 53.21. pg_event_trigger The catalog pg_event_trigger stores event triggers. See Chapter 40 for more information. Table 53.21. pg_event_trigger Columns Column Type Description oid oid Row identifier evtname name Trigger name (must be unique) evtevent name Identifies the event for which this trigger fires evtowner oid (references pg_authid .oid) Owner of the event trigger evtfoid oid (references pg_proc .oid) The function to be called evtenabled char Controls in which session_replication_role modes the event trigger fires. O = trigger fires in “origin” and “local” modes, D ="
What does this passage describe?,"shown above, we could write: SELECT c FROM inventory_item c; This query produces a single composite-valued column, so we might get output like: c ------------------------ (""fuzzy dice"",42,1.99) (1 row) Note however that simple names are matched to column names before table names, so this example works only because there is no column named c in the query's tables. The ordinary qualified-column-name syntax table_name .column_name can be understood as applying field selection to the composite value of the table's current row. (For efficiency reasons, it's not actually implemented that way.) When we write SELECT c.* FROM inventory_item c; then, according to the SQL standard, we should get the contents of the table expanded into separate columns: name | supplier_id | price ------------+-------------+------- 203"
What does this passage describe?,"pg_verifybackup Options pg_verifybackup accepts the following command-line arguments: -e --exit-on-error Exit as soon as a problem with the backup is detected. If this option is not specified, pg_veri- fybackup will continue checking the backup even after a problem has been detected, and will report all problems detected as errors. -i path --ignore=path Ignore the specified file or directory, which should be expressed as a relative path name, when comparing the list of data files actually present in the backup to those listed in the backup_man- ifest file. If a directory is specified, this option affects the entire subtree rooted at that location. Complaints about extra files, missing files, file size differences, or checksum mismatches will be suppressed if the relative path name matches the specified path name. This option can be specified multiple times. -m path --manifest-path= path Use the manifest file at the specified path, rather than one located in the root of the backup di- rectory. -n --no-parse-wal Don't attempt to parse write-ahead log data that will be needed to recover from this backup. -P --progress Enable progress reporting. Turning this on will deliver a progress report while verifying check- sums. This option cannot be used together with the option --quiet . -q --quiet Don't print anything when a backup is successfully verified. -s --skip-checksums Do not verify data file checksums. The presence or absence of files and the sizes of those files will still be checked. This is much faster, because the files themselves do not need"
What does this passage describe?,"current directory on the pri- mary, and remote_dir is above the old and new cluster directories on the standby. The directory structure under the specified directories on the primary and standbys must match. Consult the rsync manual page for details on specifying the remote directory, e.g., rsync --archive --delete --hard-links --size-only --no-inc- recursive /opt/PostgreSQL/12 \ /opt/PostgreSQL/16 standby.example.com:/opt/PostgreSQL You can verify what the command will do using rsync's --dry-run option. While rsync must be run on the primary for at least one standby, it is possible to run rsync on an upgraded standby to upgrade other standbys, as long as the upgraded standby has not been started. What this does is to record the links created by pg_upgrade's link mode that connect files in the old and new clusters on the primary server. It then finds matching files in the standby's old cluster and creates links for them in the standby's new cluster. Files that were not linked on the primary are copied from the primary to the standby. (They are usually small.) This provides rapid standby upgrades. Unfortunately, rsync needlessly copies files associated with temporary and unlogged tables because these files don't normally exist on standby servers. If you have tablespaces, you will need to run a similar rsync command for each tablespace directory, e.g.: 2225"
